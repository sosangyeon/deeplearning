Abstract 
DDPM은 적대적 훈련 없이 고품질 이미지 생성을 달성했지만, 샘플을 생성하기 위해 여러 단계에 걸쳐 마르코프 체인을 시뮬레이션해야 한다.
샘플링을 가속화 하기 위해 DDPM과 동일한 학습 절차를 사용하는 반복적인 암시적 확률 모델의 효율적인 DDIM을 제시한다. 
DDIM에서 생성 과정은 특정 마르코비안 확산 과정의 역으로 정의된다. 
훈련 목표로 이어지는 비마크로비안 확산 프로세스 클래스를 통해 DDPM을 일반화한다.
이러한 비마르코비안 프로세스는 결정론적 생성 프로세스에 해당할 수 있으며, 이로 인해 고품질 샘플을 훨씬 빠르게 생성하는 암시적 모델이 생성될 수 있다. 
DDIM이 DDPM에 비해 더 빠른 고품질 샘플을 생성할 수 있고, 잠재 공간에서 직접 의미론적으로 의미있는 이미지 보간을 수행하고, 매우 낮은 오류로 재구성할 수 있다.

1. Introduction
deep 생성 모델은 많은 영역에서 고품질 샘플을 생산할 수 있는 능력을 입증했다. 이미지 생성 측면에서 GAN은 VAEs, autoregressive models, normalizing flows과 같은 확률 기반 모델들에 비해 더 높은 샘플 품질을 보여준다.
그러나 GAN은 훈련을 안정화하기 어렵다.
DDPM 및 NCSN과 같은 반복 생성 모델에 대한 최근 연구는 adversarial training없이 GAN에 필적하는 샘플을 생성할 수 있는 능력을 보여주었다.
이를 달성하기 위해 많은 잡음 제거 자동 인코딩 모델이 다양한 수준의 가우스 노이즈에 의해 손상된 샘플의 잡음을 제거하도록 훈련된다. 
그런 다음 샘플은 점진적으로 노이즈를 제거하여 이미지로 만드는 마르코프 체인에 의해 생성된다. 
이 생성적 마르코프 체인 프로세스는 langevin dynamics를 기반으로 하거나 역 프로세스 통해 얻는다.
이러한 모델의 단점은 고품질 샘플을 생성하기 위해 많은 반복이 필요하다는 것이다. DDPM의 경우, 수천단계를 가질 수 있다.
단일 샘플을 생성하려면 모든 단계를 반복해야 하며, 이는 네트워크를 한 번만 통과하면 되는 GAN에 비해 훨씬 느리다.
예를 들어, DDPM에서 32 × 32 크기의 50k 이미지를 샘플링하는 데 약 20시간이 걸리지만 Nvidia 2080 Ti GPU의 GAN에서 샘플링하는 데는 1분도 채 걸리지 않는다.
DDPM과 GAN 간의 이러한 효율성 격차를 줄이기 위해 DDIM(Denoising Diffusion Implicit Model)을 제시한다.
DDPM에서 사용하는 순방향 확산 과정(Markovian)을 non-Markovian 과정으로 일반화하며, 이에 대해 여전히 적합한 역 생성 마르코프 체인을 설계할 수 있다.
DDIM과 DDPM이 모델을 학습하는 과정에서 동일한 목적 함수를 최소화한다.
따라서 동일한 신경망을 사용하는 대규모 생성 모델 제품군에서 다른 비마르코비안 확산 과정과 해당 역 생성 마르코프 체인을 선택하기만 하면 자유롭게 선택할 수 있다.
특히, 적은 수의 단계로 시뮬레이션할 수 있는 '짧은' 생성적 마르코프 체인으로 이어지는 비마르코프 확산 프로세스를 사용할 수 있다.
이점
1. DDIM은 샘플링을 가속화할때 DDPM에 비해 우수한 샘플 생성 품질을 제공한다.
2. DDIM 샘플은 일관성 속성을 가지고 있다. 동일한 초기 잠재 변수로 시작하여 다양한 길이의 마르코프 체인을 가진 여러 샘플을 생성하면 유사한 고수준 기능을 갖게 된다.
3. DDIM의 일관성으로 인해 확률적 생성 과정으로 인해 이미지 공간 근처에서 보간하는 DDPM과 달리 DDIM의 초기 잠재 변수를 조작하여 의미론적으로 의미 있는 이미지 보간을 수행할 수 있다.
(DDIM은 초기 잠재 변수(latent variable)를 조작하여 보다 일관된 방식으로 이미지를 생성할 수 있다. 여기서 일관성이란, 잠재 공간에서의 작은 변화가 이미지 공간에서 의미 있는 변화를 일으키는 것을 의미한다. 
이 특성은 DDIM이 특정 잠재 변수 조작을 통해 의미론적으로 유의미한 보간(즉, 한 이미지에서 다른 이미지로 자연스럽게 변형하는 과정)을 할 수 있음을 나타낸다.)

2. Background
데이터 분포 q(x0)로부터 샘플이 주어지면, q(x0)에 근사하고 표본을 추출하기 쉬운 모델 분포pθ(x0)를 학습하는 데 관심이 있다.
x1...xt는 x0와 동일한 샘플 공간에 있는 잠재 변수이다. 

3 VARIATIONAL INFERENCE FOR NON-MARKOVIAN FORWARD PROCESSES
