1. Introduce
계산 시간의 단계에 위치를 정렬하면 이전의 은닉 상태 ht-1과 위치 t에 대한 입력의 함수로 일련의 은닉 샅애 ht가 생성된다. 
이러한 본질적으로 순차적인 특성은 훈련 예제 내에서 병렬화를 배제하며, 이는 메모리 제약 조건이 예제 간 일괄 처리를 제한하기 때문에 더 긴 시퀀스 길이에서 중요해진다.
최근 연구에서는 factorization 기법과 조건부 계산을 통해 계산 효율성이 크게 향상되었으며 조건부 계산의 경우 모델 성능도 향상되었다. 
그러나 순차 계산의 근본적인 제약은 여전히 남아 있다.
어텐션 메커니즘은 다양한 작업에서 매력적인 시퀀스 모델링 및 transduction 모델의 필수적인 부분이 되어쓰며, 입력 또는 출력 시퀀스에서의 거리에 관계없이 종속성을 모델링할 수 있다.

2. Background
순차 계산을 줄이는 목표는 또한 확장 신경 GPU, ByteNet 및 ConvS2S의 기초를 형성하며, 모두 컨볼루션 신경망을 기본 빌딩 블록으로 사용하여 모든 입력 및 출력 위치에 대해 병렬로 숨겨진 표현을 계산한다.
이러한 모델에서 두 개의 임의의 입력 또는 출력 위치의 신호를 연결하는데 필요한 연산 수는 위치 사이의 거리에 따라 증가한다.
이로 인해 먼 위치 간의 종속성을 학습하기가 더 어려워진다.
트랜스포머에서는 이 작업이 일정한 수의 연산으로 줄어들지만, 평균 어텐션 가중치 위치로 인해 유효 해상도가 감소하는 대가(모델이 각 단어에 대해 구체적인 위치 정보를 덜 정확하게 학습할 수 있게 된다. 
즉, 특정 단어에 집중하지 않고, 주목이 고르게 분산될 수 있다.)를 치르게 되며 이는 멀티 헤드 어텐션으로 상쇄된다.
셀프 어텐션은 시퀀스의 표현을 계산하기 위해 단일 시퀀스의 다른 위치를 관련시키는 어텐션 메커니즘이다.

3. Model Architecture
인코더는 입력 시퀀스(x)를 연속 표현 시퀀스(z)에 매핑한다.
z가 주어지면 디코더는 한 번에 한 요소씩 기호의 출력 시퀀스(y)를 생성한다. 각 단계에서 모델은 auto-regressive이며, 다음 단계를 생성할 때 이전에 생성된 기호를 추가 입력으로 사용한다.

3.1 ncoder and Decoder Stacks
인코더는 N=6개의 동일한 레이어 스택으로 구성된다. 
각 레이어에는 두 개의 하위 레이어가 있다. 첫 번째는 멀티 헤드 셀프 어텐션이고, 두 번째는 간단한 positionwise fully connected feed forward network이다.
두 개의 하위 레이어 각각에 residaul connection을 사용한 다음 레이어 정규화를 사용한다. 
즉, 각 하위 계층의 출력값은 layernorm(x+sublayer(x))이다. 이러한 residual connection을 용이하게 하기 위해 모델의 모든 하위 레이어와 임베딩 레이어는 512차원의 출력을 생성한다.
디코더는 N=6개의 동일한 레이어 스택으로 구성된다.
각 인코더 계층에 있는 두 개의 하위 계층 외에도 디코더는 인코더 스택의 출력에 대해 멀티 헤드 어텐션을 수행하는 세 번째 하위 계층을 삽입한다.
인코더와 유사하게, 각 하위 레이어 주위에 residual connection을 사용한 다음 레이어 정규화를 사용한다.
마스킹은 출력 임베딩이 한 위치만큼 오프셋된다는 사실과 결합되어 위치 i에 대한 예측이 i보다 작은 위치의 알려진 출력에만 의존할 수 있도록 한다.

3.2 Attention
어텐션 함수는 쿼리와 키 값 쌍 세트를 출력에 매핑하는 것으로 설명할 수 있다. 여기서 쿼리 키 및 출력은 모두 벡터이다. 
쿼리(Query), 키(Key), 밸류(Value):

어텐션 메커니즘에서 입력 데이터는 쿼리(Query), 키(Key), **밸류(Value)**라는 세 가지 벡터로 표현된다.
쿼리는 모델이 특정 정보(예: 문장에서 다음에 올 단어)에 대해 "질문"하는 역할을 한다.
키는 쿼리와 비교되는 벡터로, 입력 데이터의 각 부분이 가지고 있는 정보를 나타낸다.
밸류는 키와 연관된 데이터로, 최종 출력에 반영될 정보이다.
출력 계산 방식:
출력은 **밸류(Value)**들의 가중합으로 계산된다. 이 가중치는 쿼리와 각 키 사이의 **유사도(compatibility)**를 기반으로 한다.
유사도는 주로 쿼리와 키의 내적(dot product)으로 계산되며, 이 값이 클수록 해당 밸류에 더 높은 가중치가 부여한다.
1. 쿼리, 키, 밸류 벡터 설정
쿼리 벡터 
Q: [1, 0, 1]

키 벡터 
K1 : [1, 0, 0]
K2 : [0, 1, 0]
K3 : [1, 1, 0]

밸류 벡터 
V1 : [0.1, 0.2]
V2 : [0.0, 0.3]
V3 : [0.4, 0.5]

2. 쿼리와 키의 내적 계산
𝑄와 Ki간의 내적(dot product)을 계산하여 어텐션 스코어를 구한다.
Q⋅K1 =[1,0,1]⋅[1,0,0]=1×1+0×0+1×0=1
Q⋅K2 =[1,0,1]⋅[0,1,0]=1×0+0×1+1×0=0
Q⋅K3 =[1,0,1]⋅[1,1,0]=1×1+0×1+1×0=1

3. 스케일링 및 소프트맥스 적용
내적 결과를 키 벡터 차원의 제곱근으로 나눠 스케일링하고, 소프트맥스 함수를 적용하여 확률 분포로 변환한다.

스케일링: 차원의 제곱근이 3이므로, 각 내적 결과를 3으로 나눈다.

Scaled Score
Scaled Score1 = 1/3^0.5
​Scaled Score2 = 0
Scaled Score3 = 1/3^0.5

소프트맥스 적용:
소프트맥스 함수는 각 스코어에 대해 
exp(Scaled Score𝑖)를 계산하고, 이를 모두 더한 값으로 나눈다.
Attention Weight𝑖=exp(Scaled Score𝑖)∑𝑗exp(Scaled Score𝑗)

4. 밸류 벡터의 가중합 계산
각 밸류 벡터를 어텐션 가중치로 가중합하여 최종 출력을 계산한다.
출력 = Attention Weight1×𝑉1 + Attention Weight2×𝑉2 + Attention Weight3×𝑉3

3.2.1 Scaled Dot-Product Attention


