## Neural Discrete Representation Learning

## Abstract
이산 표현을 학습하는 간단하지만 강력한 생성 모델을 제안
VQ-VAE는 두가지 주요 면에서 VAE와 다르다.

(autoregressive decoder : 데이터를 순차적으로 생성하는 강력한 디코더이다. 
예를 들어, 문장이나 이미지를 생성할 때 이전에 생성된 요소를 바탕으로 다음 요소를 예측하고 생성한다.)
autoregressive decoder는 매우 강력하기 때문에, 인코더가 생성한 잠재 변수를 무시하고도 충분히 좋은 데이터를 생성할 수 있다. 
이 경우 인코더가 학습 과정에서 잠재 변수를 제대로 활용하지 않게 되어, 잠재 변수가 의미를 상실하는 '사후 붕괴' 현상이 발생한다.

1. Introduction
원시 데이터(예: 이미지, 텍스트, 음성 등)의 중요한 특징들을 추출하여 이를 간단한 형태로 변환한다. 
어려운 작업을 잘 수행하려면, 데이터로부터 학습된 표현이 매우 중요하지만 
비지도 학습 방식으로 훈련된 표현은 여전히 머신러닝 분야에서 주류 접근 방식이 되지 못하고 있다.

최대 우도(Maximum Likelihood): 최대 우도는 모델이 주어진 데이터를 가장 잘 설명할 수 있도록 모델의 파라미터를 조정하는 방법
예를 들어, 이미지 생성 모델이 특정 이미지를 생성할 확률이 최대가 되도록 학습시키는 것이다.
쉽게 말해, 모델이 학습 데이터를 관찰할 가능성(likelihood)을 최대화하는 방향으로 학습한다.

재구성 오류(Reconstruction Error): 재구성 오류는 입력 데이터를 모델에 넣어서 다시 원래 데이터로 복원했을 때의 차이를 측정하는 것
예를 들어, 오토인코더(autoencoder) 모델은 이미지를 압축한 후 다시 복원한다. 원래 이미지와 복원된 이미지 사이의 차이가 재구성 오류이다.
재구성 오류를 최소화하는 것은 모델이 원래 데이터의 중요한 특징을 잘 유지하도록 하는 목표이다.

 이 방법들이 얼마나 유용한지는 그 모델이 어떤 응용 프로그램에 사용되는지에 따라 달라질 수 있다.
예를 들어, 이미지 생성 작업에서는 최대 우도가 중요할 수 있다. 왜냐하면, 모델이 생성한 이미지가 실제 데이터와 최대한 유사해야 하기 때문이다.
반면에, 데이터 압축이나 노이즈 제거와 같은 응용에서는 재구성 오류를 최소화하는 것이 더 중요할 수 있다. 원래 데이터를 얼마나 정확하게 복원할 수 있는지가 관건이기 때문이다.

-> 목표는 잠재 공간에서 데이터의 중요한 기능을 보존하면서 최대한의 가능성을 위해 최적화하는 모델을 달성하는 것

PixelCNN 같은 모델은 잠재 변수를 사용하지 않고도 훌륭한 성능을 낼 수 있는 생성 모델이다. 
하지만 다양한 영여겡서 불연속적이고 유용한 잠재 변수를 학습해야 한다. 즉, 이산 표현에 집중해야 한다.
이산 표현은 복잡한 추론, 계획 및 예측 학습에 자연스럽게 적합하다. 

VQ-VAE는 잠재 공간을 효과적으로 활용할 수 있기 때문에 국소적인 노이즈와 인지할 수 없는 세부 사항에 집중하는 것과는
대조적으로 데이터 공간의 여러 차원에 걸쳐 있는 중요한 특징을 성공적으로 모델링할 수 있다. 

2. Related Work
discrete latent variables를 사용하여 variational autoencoders를 학습하는 방법을 제시한다.
discrete variables를 딥러닝에 적용하는 것은 어려운 것으로 입증되어있다. - 기본 양식이 본질적으로 불연속적인 경우에도

(변분 오토인코더(VAE)에서의 목표 함수는 주로 ELBO를 최적화하는 것이다. 
single-sample objective : VAE에서 학습할 때, latent variable을 한 번 샘플링하여 이 샘플로부터 목표 함수를 계산하는 방법
이 방법은 계산이 단순하지만, 샘플링된 z에 따라 결과가 크게 달라질 수 있다. 즉, 그라디언트의 변동성(분산)이 커져 학습이 불안정해질 수 있다.
multi-sample objective : 잠재 변수 z를 여러 번 샘플링하고, 각 샘플에 대해 목표 함수를 계산한 후, 이를 평균내어 최종 목표 값을 계산한다.)

이산적 잠재 변수와 학습의 어려움
잠재 변수가 이산적(discrete)일 경우, 그라디언트 기반의 최적화가 어려워진다. 이산 변수는 갑작스러운 변화를 가지며, 미분 가능하지 않기 때문에 그라디언트를 계산하기 어렵다.
Continuous Reparameterization : 재매개변수화는 모델이 이산적인 문제를 연속적인 문제로 바꿔서 처리하는 기법이다.
Concrete, Gumbel-softmax 분포
검벨-소프트맥스(Gumbel-softmax)는 검벨(Gumbel) 분포에서 샘플링한 값들에 소프트맥스(softmax) 함수를 적용하여 얻는 분포이다.
이를 통해 이산적 선택 문제를 연속적 값으로 근사할 수 있다.
콘크리트 분포는 이와 유사하게 이산적 변수를 연속적으로 표현하여 학습 가능한 형태로 만든다.

온도 상수는 확률 분포의 "부드러움"을 조절한다. 온도가 높을수록 분포는 더 부드러워지고, 샘플들이 더 균등하게 분포된다. 반면, 온도가 낮아질수록 분포는 날카로워지고, 특정 값에 더 집중된다.
학습 과정에서 온도 상수를 점차 낮추는 과정을 '어닐링(annealing)'이라고 한다. 

Gaussian reparameterization trick은 연속적인 잠재 변수에 대해 그라디언트를 효율적으로 계산할 수 있게 한다. 이를 통해 모델의 학습 과정에서 발생하는 그라디언트의 분산(gradient variance)이 매우 작아져, 더 안정적인 학습이 가능하다.

Scalar Quantization는 데이터를 압축하는 기본적인 방법 중 하나이다. 양자화는 데이터를 일정한 범위 내의 값들로 근사하는 과정으로, 이 과정을 통해 데이터의 크기를 줄이거나 압축할 수 있다.
스칼라 양자화는 데이터를 특정한 범위나 단계(step)로 나누고, 각 데이터를 해당 범위의 대표 값으로 변환한다.

Arithmetic Encoding은 데이터를 매우 효율적으로 압축할 수 있는 엔트로피 부호화(entropy coding) 기법 중 하나이다.
특히 데이터의 발생 확률에 따라 데이터를 가변 길이의 비트열로 인코딩하는 방법이다. 발생 확률이 높은 데이터는 짧은 비트열로, 발생 확률이 낮은 데이터는 긴 비트열로 표현되어 전체적으로 압축이 잘 된다.

Vector Quantization 과정 
코드북(Codebook) : 벡터 양자화의 핵심은 코드북이라는 데이터베이스를 만드는 것이다. 코드북은 미리 정의된 벡터들의 집합으로, 원본 데이터 벡터들을 가장 잘 대표할 수 있는 벡터들의 모음이다.
양자화(Quantization): 벡터 양자화에서는 원본 벡터를 코드북에 있는 가장 가까운 벡터로 대체한다. 이를 통해 원래의 벡터 공간을 코드북에 있는 벡터들로 분할하여, 각각의 입력 벡터를 가장 가까운 코드북 벡터로 매핑다.
압축(Compression): 벡터 양자화 과정을 통해 입력 데이터는 코드북의 인덱스로 대체되므로, 데이터가 압축된다. 즉, 원래 데이터 대신 코드북에서 선택된 벡터의 인덱스만 저장한다.

hard clustering :  각 데이터 포인트가 하나의 클러스터에만 속할 수 있다. 즉, 데이터 포인트가 특정 클러스터에 할당되면, 다른 클러스터에는 속할 수 없다.
어닐링(annealing) 과정은 벡터 양자화에서 사용된 연속적 이완(continuous relaxation)을 점차 하드 클러스터링(hard clustering)으로 변환하는 과정을 의미한다.

오토인코더와 벡터 양자화를 결합하여 신경망을 훈련하는 방법
1. 오토인코더 학습
2. 벡터 양자화(Vector Quantization) 적용 - 인코더의 출력을 사전에 정의된 코드북의 벡터들로 양자화하는 과정
3. Soft-to-Hard Relaxation - 네트워크 전체를 작은 학습률로 다시 훈련시켜, 소프트에서 하드로 전환하는 과정을 안정화

실험에서의 문제:
실험 과정에서 저자들이 발견한 문제는 디코더가 연속적인 이완(continuous relaxation)을 역전시킬 수 있다는 것이었다.
(즉, 양자화가 제대로 작동하지 않았고, 디코더가 연속적인 값으로 돌아가 버렸기 때문에 실제로 원하는 대로 이산적 데이터를 만들지 못했다는 의미이다.)

3. VQ-VAE
VAE는 입력 데이터 x가 주어졌을 때 이산 잠재 확률 변수 z의 사후 분포 q(z|x)를 매개변수화하는 인코더 네트워크, 사전 분포 p(z), 입력 데이터에 대한 분포 p(x|z)를 갖는 디코더로 구성된다.
VAE의 사후 및 사전 분포는 대각선 공분산으로 정규 분포를 따르는 것으로 가정되며, 이를 통해 가우스 재매개변수화 기법을 사용할 수 있다.
VQ_VAE에서는 사후 분포와 사전 분포는 categorical(이산적)이며, 이러한 분포에서 추출한 표본은 임베딩 테이블을 인덱싱한다.
(Posterior Distribution에서 범주형 값을 샘플링한다. 샘플링된 이 범주형 값은 임베딩 테이블에서 특정 위치를 가리키는 인덱스 역할한다. 
임베딩 테이블은 각 범주에 대한 고차원 벡터(임베딩)를 저장한 데이터베이스이다.)

3.1 Discrete Latent variables
e∈R{K×D} 잠재 임베딩 공간 정의
K는 이산 잠재 공간(K-way 범주형)의 크기이고 D는 각 잠재 임베딩 벡터 ei의 차원이다.

입력 x를 인코더에 통과시키면 z_e(x)라는 출력이 생성된다. 이는 연속적인 값으로 표현된 벡터이다.
생성된 ze(x)와 임베딩 공간에 있는 K개의 벡터들 간의 거리를 계산하여, 가장 가까운 벡터 ek를 찾는다. 
이 벡터가 이산적인 잠재 변수 z에 해당한다.
찾아낸 임베딩 벡터 𝑒𝑘는 디코더에 입력으로 사용되어 데이터를 복원하는 데 활용된다.

3.2 Learning
양자화는 이산적인 값으로 변환하는 과정이므로, 이 과정에서 그라디언트를 계산하는 것이 어려운 문제가 된다.
Straight-through estimator : 그래디언트를 근사하여, 디코더 입력에서 인코더 출력로 그래디언트를 단순히 복사하는 방식
즉, 양자화 단계에서 미분이 불가능하지만, 그래디언트 계산을 우회하는 방법으로 학습을 가능하게 한다.

역방향 계산(그래디언트 전파) : 디코더의 출력에 대한 그래디언트는 인코더로 그대로 전달된다.
인코더 출력 ze(x)와 디코더 입력 zq(x)가 같은 차원 공간에서 표현되므로, 재구성 손실을 줄이기 위해 인코더가 어떻게 출력을 조정해야 하는지에 대한 정보를 그래디언트가 포함하고 있다.

VQ-VAE의 세가지 구성 요소
재구성 손실: 디코더가 원본 입력을 얼마나 잘 복원하는지를 평가하며, 인코더와 디코더 모두 최적화된다.
임베딩 손실: 임베딩 벡터를 인코더 출력에 가깝게 이동시키며, 임베딩만 최적화된다.
커밋먼트 손실: 인코더가 임베딩 벡터에 더 잘 맞추도록 유도한다.

L = log p(x|zq(x)) + ksg[ze(x)] − ek22 + βkze(x) − sg[e]k22 (sg : stop-gradient)

재구성 손실
재구성 손실항에서 Straight-through gradient estimation으로 임베딩 ei가 그래디언트를 받지 않는다.
따라서 임베딩 공간을 학습하기 위해 가장 간단한 사전 학습 알고리즘 중 하나인 VQ(Vector Quantisation)를 사용한다.

임베딩 손실
임베딩 벡터ei를 인코더 출력 ze(x) 쪽으로 이동

커밋먼트 손실
만약 인코더가 임베딩 벡터에 충분히 빠르게 맞춰지지 않으면, 인코더가 출력하는 값이 계속 커져서 임베딩 벡터와의 거리가 커질 수 있다.
이로 인해, 임베딩 공간이 계속 확장되어 모델이 불안정해질 수 있다.

디코더는 재구성 손실항에, 인코더는 재구성 손실, 커밋먼트 손실에 의해 최적화되며, 임베딩은 임베딩 손실에 의해 최적화된다.

β는 커밋먼트 손실의 가중치이다. β 값이 0.1에서 2.0 사이에서 결과에 큰 영향을 주지 않았기 때문에, β 값에 대해 알고리즘이 강건하다는 결론을 얻었다. 모든 실험에서는 β = 0.25를 사용했다.

디코더가 완전히 수렴한 후에는, 
z가 

zq(x)와 다를 때 p(x∣z)에 확률을 할당하지 않아야 한다. 즉, 디코더는 zq(x)로만 데이터를 재구성할 수 있도록 학습된다.
-> log p(x) ≈ log p(x|zq(x))p(zq(x)).

Jensen's Inequality에 따라 log p(x) ≥ log p(x|zq(x))p(zq(x)).
