# Normalization

정규화의 목적은 네트워크의 forward 단계에서 모든 데이터의 스케일을 동일하게 만들어 
출력값을 안정적으로 만드는 것이다.

만약 이전 층인 X(t-1) 레이어에서 각기 다른 분포와 편향된 평균을 가진 출력값들이 전파된다면, 
그 결과는 방향성이 일정하지 않고 불안정할 수 있다. 이는 학습 과정에서 네트워크가 일관된 학습을 하기 어렵게 만들며, 
학습 속도와 성능 저하로 이어질 수 있다.

정규화는 이를 해결하기 위해 평균을 0, 분산을 1로 맞추어 데이터의 스케일을 조정한다.
이렇게 함으로써 네트워크의 모든 층에서 입력값의 분포가 일정해져 학습이 안정되고, 
학습 속도가 빨라지며, 최적화 과정에서의 난제가 줄어들게 된다. 

대표적인 정규화 기법으로는 배치 정규화(Batch Normalization), 층 정규화(Layer Normalization), 인스턴스 정규화(Instance Normalization) 등이 있다.

## Batch Normalization
[44,1,4096,3] 배치 1개는 3차원 정보를 가진 4096개의 데이터가 존재한다. 
배치 정규화는 dim = 3을 기준으로 정규화를 진행한다. (3개의 평균과 분산)

## Layer Normalization 
[44,1,4096,3] 배치 1개는 3차원 정보를 가진 4096개의 데이터가 존재한다. 
레이어 정규화는 dim = 2를 기준으로 정규화를 진행한다. (4096개의 평균과 분산)
장점
-> 배치 정규화는 작은 배치 크기에서 극단적 결과를 내는데 반해, 작은 배치 사이즈에서도 효고적인 이용이 가능
-> 시퀀스에 따른 고정길이 정규화로 배치 정규화에 비해 RNN 모델에 더 효과적인 방법이다.
단점
-> 추가 계산 및 메모리 발생
## Instance Normalization


