Abstract
원시 포인트 클라우드에서 3D 객체 감지를 위한 PointRCNN을 제안한다.
전체 프레임워크는 1. bottom-up 3D proposal generation 그리고 2.Refining Proposals in Canonical Coordinates
이전 방법처럼 RGB 이미지에서 proposals을 생성하거나 포인트 클라우드를 bev 또는 복셀에 투영하는 대신, 
스테이지 1 하위 네트워크는 전체 장면의 포인트 클라우드를 전경 포인트와 배경으로 분할하여 상향식 방식으로 포인트 클라우드에서 소수의 고품질 3D proposal을 직접 생성한다.
(Foreground Points : 실제 객체가 포함되어 있는 포인트)
(Background Points : 포인트 클라우드에서 배경을 구성하는 포인트)

1. Introduction
2D 장면 이해를 넘어 3D 물체 감지는 자율 주행 및 가정용 로봇과 같은 많은 실제 응용 분야에서 중요하고 필수적이다. 
최근에 개발된 2D 감지 알고리즘은 이미지의 다양한 시점과 배경 혼란을 처리할 수 있지만, 포인트 클라우드를 사용한 3D 물체 감지는 불귳ㄱ한 데이터 형식과 
3D 물체의 6DoF의 넓은 검색 공간으로 인해 여전히 큰 도전에 직면했다.
SOTA 방법은 BEV, 정면뷰 또는 일반 3D 복셀에 투영하여 성숙한 2D 감지 프레임워크를 활용하는데, 이는 최적이 아니며 양자화 중에 정보 손실이 발생한다.
3D 감지에 pointnet을 적용하여 2D RGB 감지 결과에서 잘린 포인트 클라우드를 기반으로 3D 경계 상자를 추정했다. 
그러나 이 방법의 성능은 2D 검출 성능에 크게 의존하며 강력한 바운딩 박스 제안을 생성하기 위한 3D 정보의 이점을 활용할 수 없다.

3D 객체 감지를 위한 훈련 데이터는 3D 객체 분할을 위한 시맨틱 마스크를 직접 제공한다. 이것이 3D 검출과 2D 검출 훈련 데이터의 주요 차이점이다. 
제안된 프레임워크는 2단계로 구성되며, 첫 번째 단계는 상향식 체계에서 3D 바운딩 박스 제안을 생성하는 것을 목표로 한다. 
첫 번째 단계에서는 3D 바운딩 박스를 사용하여 실측 분할 마스크를 생성함으로써 전경 포인트를 세그먼트화하고 세그먼트 포인트에서 동시에 소수의 바운딩 박스 proposal을 생성한다.
이러한 전략은 이전 방법처럼 전체 3D 공간에서 많은 수의 3D 앵커 박스를 사용하지 않고 많은 계산을 절약한다. 
pointrcnn의 두 번째 단계에서는 표준 3D 박스 미세 조정을 수행한다.


