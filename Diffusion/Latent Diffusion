## Abstract
확산 모델은 이미지를 생성하는 과정을 여러 단계의 노이즈 제거 오토인코더로 나누어 처리함으로써, 고품질의 이미지를 합성할 수 있다.
또한, 그들의 formulation은 retraining없이 이미지 생성 프로세스를 제어할 수 있는 guiding mechanism을 허용한다.

그러나 이 모델(DM)은 일반적으로 픽셀 공간에서 직접 작동하기 때문에 강력한 DM을 최적화하는데 수백 일의 GPU일이 소요되는 경우가 
많으며 순차적 평가로 인해 추론 비용이 많이 든다.
-> 사전 훈련된 오토인코더의 잠재 공간에서 디노이징을 수행해 계산 복잡도를 줄이면서 이미지 품질을 유지한다.

모델 아키텍쳐에 cross attention 레이어를 적용함으로써 입력 조건과 이미지 생성 과정 사이의 관계를 학습하여, 
조건에 맞는 이미지를 생성할 수 있도록 돕는다.

1. Introduction
이미지 합성이 최근 컴퓨터 비전 분야에서 크게 발전했지만, 그만큼 높은 계산 요구를 필요로 한다.
특히, 복잡한 고해상도 이미지를 합성하는 작업은 Autoregressive (AR) Transformers와 같은 확률 기반 모델이 주도하고 있으며, 
이 모델들은 수십억 개의 매개변수를 포함할 수 있습니다. 
GANs는 고해상도 이미지 생성에서 뛰어난 성과를 보였지만, 그 학습 방식의 한계로 인해 복잡하고 다양한 데이터 분포를 잘 처리하지 못한다.

DM은 매개변수 공유를 크게 활용하기 때문에, Autoregressive (AR) 모델처럼 수십억 개의 매개변수를 포함하지 않으면서도 복잡한 자연 이미지 분포를 모델링할 수 있다.
우도 기반 모델의 부류의 특징인 모든 가능한 모드(즉, 다양한 패턴과 세부 사항)를 커버하려는 경향이 있어 
사람이 인식할 수 없는 작은 세부 사항을 모델링하는 데 많은 계산 자원을 소비하는 경향이 있다. 
(예를 들어, 단일 A100 GPU에서 50k 샘플을 생성하는데 약 5일이 걸린다.)

영향
1. 막대한 컴퓨팅 리소스가 필요하고 막대한 탄소 발자국을 남긴다. 
2. 이미 훈련된 모델을 평가하는 것은 동일한 모델 아키텍처가 많은 수의 단계에 대해 
순차적으로 실행되어야 하기 때문에 시간과 메모리 비용도 많이 든다. 
-> 접근성을 높이는 동시에 리소스 소비를 줄이려면 학습과 샘플링에 대한 계산 복잡성을 줄이는 방법이 필요하다. 

Departure to Latent Space 
학습은 두 단계로 나눌 수 있다. 
첫 번째 단계 - perceptual compression : 이미지의 고주파 세부 사항(작고 복잡한 디테일)을 제거하면서 중요한 요소들을 유지한다.
두 번째 단계 - semantic compression : 모델이 이미지의 주요 구조와 의미를 이해하고 재구성하는 과정
이 두 단계를 통해, 더 효율적이고 계산적으로 적합한 잠재 공간에서 고해상도 이미지 합성을 위한 확산 모델을 훈련한다.

일반적인 관행에 따라 오토인코더를 훈련한다. 중요한 것은 학습된 잠재 공간에서 DM을 훈련하기 때문에 과도한 공간 압축에 의존할 필요가
없으며 이는 공간 차원에 대해 더 나은 스케일링 속성을 나타낸다. 
(Scaling properties : 모델이 데이터의 크기나 복잡성이 증가할 때 어떻게 성능을 유지하거나 개선할 수 있는지를 설명하는 개념이다.
좋은 스케일링 속성을 가진 모델은 처리해야 할 데이터 양이 증가하더라도 성능이 급격히 떨어지지 않고, 효과적으로 작동할 수 있다.)
또한 복잡성이 감소하여 단일 네트워크 패스로 잠재 공간에서 효율적인 이미지 생성이 가능하다. 

LDM에서는 자동 인코더를 한 번만 훈련하면, 이를 다양한 Diffusion Model (DM) 학습에 재사용할 수 있다.
이는 새로운 작업을 시작할 때마다 처음부터 모델을 훈련할 필요 없이, 이미 학습된 자동 인코더를 활용하여 효율적으로 다른 작업을 수행할 수 있다.
트랜스포머를 DM의 UNET 백본에 연결하고 임ㅇ의의 유형의 토큰 기반 컨디셔닝 메커니즘을 가능하게 하는 아키텍처를 설계한다.

기존 연구는 인코더/디코더 아키텍처와 점수 기반 사전(확률 분포)을 동시에 학습시키며, 재구성과 생성 능력 사이의 미세한 균형을 맞추는 것이 필요했다. 
하지만 이 연구에서는 그러한 복잡한 조정 없이도 정확한 재구성을 제공하고, 잠재 공간을 추가로 정규화할 필요가 거의 없다고 주장한다.

2. Related Work
GAN : 품질이 우수하고, 최적화가 어렵고 전체 데이터 분포를 캡처하는데 어려움이 있다.
VAE : 우도 기반 모델은 최적화를 더 잘 동작한다. 생성품질이 GAN에 비해 떨어질 수 있다.
ARM : 강력한 밀도 추정 성능을 가지지만, 계산 비용이 많이 들고, 낮은 해상도의 이미지로 제한된다.

Two-Stage Image Synthesis
개별 생성적 접근법의 단점을 완화하기 위해 많은 연구에서 2단계 접근법을 통해 다양한 방법의 장점을 보다 
효율적이고 성능이 뛰어난 모델로 결합하는 방법을 연구했다.
VQ-VAE는 autoregressive 모델을 사용해 이산화된 잠재 공간에서 표현력이 뛰어난 사전 분포를 학습한다.
이산화된 이미지와 텍스트 표현에 대한 공동 분포를 학습해 text2image로 확장한다.
그러나 높은 압축률로 인해 성능이 제한되며, 이를 해결하기 위해 LDM은 컨볼루션 백본을 사용하여 더 고차원적인 잠재 공간으로 확장한다.

3. Method



