Abstract
포인트 클라우드에서의 객체 감지는 자율주행과 같은 많은 로봇 응용 분야에서 중요한 측면이다. 
최근 문헌에서는 두 가지 유형의 인코더를 제안한다.
고정 인코더는 빠르지만 정확도가 떨어지는 경향이 있는 반면, 데이터에서 학습된 인코더는 더 정확하지만 속도가 느리다. 
(fixed encoder : 데이터로부터 학습되지 않은 사전 정의된 방법으로 입력 데이터를 인코딩하는 알고리즘이나 기법)
이 작업에서는 pointnets을 사용하여 수직 기둥으로 구성된 포인트 클라우드의 표현을 학습하는 새로운 인코더인 pointpillars를 제안한다. 
라이다만을 사용했음에도 불구하고 최첨단 성능을 달성한다.
이 감지 성능은 62Hz에서 실행되는 동안 달성되며, 이는 2-4배 향상된 런타임이다. 

1. Introduction
이를 달성하기 위해 자율 주행 차량은 여러 센서에 의존한다. 
전통적인 라이다 기반 로보틱스 시스템에서는, 먼저 포인트 클라우드에서 배경을 제거하고 객체를 감지한 후, 감지된 객체들을 클러스터링하고, 마지막으로 이 객체들을 분류하여 무엇인지를 식별하는 과정을 거친다.
1) 포인트 클라우드는 희소 표현이고 이미지는 조밀하며 2) 포인트 클라우드는 3D이고 이미지는 2D이다. 
결과적으로, 포인트 클라우드에서의 객체 감지는 표준 이미지 컨볼루션에 적합하지 않다.

초기 작업은 3D convolution 또는 포인트 클라우드를 이미지에 투영하는 기법에 초점을 맞추었다.
최근의 방법은 BEV에서 라이다 포인트 클라우드를 보는 경향이 있다.
이 뷰는 스케일의 모호성과 다른 물체에 가려짐이 적다는 몇 가지 이점을 제공한다.
그러나 라이다 포인트 클라우드는 각 지점(포인트)들이 공간에 흩어져 있는 형태로, 특히 멀리 있는 객체나 빈 공간을 포함할 때 이러한 데이터가 매우 희박하게 된다.
희박하게 퍼져 있기 때문에, CNN을 직접 적용하면 대부분의 필터가 유의미한 정보를 얻지 못하고 빈 공간을 처리하는 데 낭비되는 경우 많다.
이 문제에 대한 일반적인 해결 방법은 지표 평면을 일반 그리드로 분할한 다음, 각 그리드 셀의 포인트에 대해 피처 인코딩을 수행하는 것이다.
pointnet 설계를 기반으로 voxelnet은 이 영역에서 진정한 종단 간 학습을 수행하는 최총의 방법 중 하나였다. 
voxelnet은 공간을 복셀로 나누고, 각 복셀에 pointnet을 적용한 다음, 3d 컨볼루션 중ㅇ간 레이어를 적용하여 z축을 통합한 다음, 2d convolution detection 아키텍처를 적용한다.
voxelnet 성능은 강력하지만 4.4Hz의 추론 시간은 실시간으로 배포하기에 너무 느리다.
최근 second는 voxelnet의 추론 속도를 개선했지만 3d convolution은 여전히 병목 현상으로 남아 있다.

이 연구에서는 2d convolution만으로 end-to-end 학습을 가능하게 하는 3d 객체 감지 방법인 pointpillars를 제안한다.
장점 
1. 학습 가능한 인코더 pointpillar는 포인트 클라우드가 나타내는 전체 정보를 활용할 수 있다. 
2. 복셀 대신 기둥에서 작동하므로 복셀 방식에서 필요한 z축 방향으로 공간을 나누는 작업(비닝)을 수동으로 설정하지 않아도 된다.
3. pillar는 모든 주요 작업을 gpu에서 계산하는데 매우 효율적인 2d convolution으로 공식화할 수 있기 때문에 매우 효율적이다. 
4. pointpillar가 다른 포인트 클라우드 구성을 사용하기 위해 수동으로 조정할 필요가 없다는 것이다. 

1.1 Related Work
1.1.1 object detection using CNNs
two-stage approach 
첫 번째 단계: Region Proposal Network (RPN)이라는 네트워크가 입력 이미지에서 후보 영역(객체가 있을 가능성이 있는 영역)을 제안한다.
두 번째 단계: 제안된 영역들을 잘라내고(resize) 네트워크에 입력하여 각각의 영역이 특정 클래스(예: 사람, 자동차 등)에 속하는지 분류한다.
one-stage approach
이미지 전역에 걸쳐 밀도 높은 앵커 박스(anchor boxes)를 생성한 다음, 이를 한 번에 회귀(regress) 및 분류하여 객체를 감지하는 방법
이 논문에서는 단일 스테이지 방법을 사용한다.

1.1.2
3d convolution은 속도가 느리다는 단점이 있다. 
3d 포인트 클라우드를 지표 평면 또는 이미지 평면에 투영하여 런타임을 향상시킨다. 
3D 포인트 클라우드를 기존 이미지 처리 네트워크에서 처리할 수 있도록 변환한다. 
pointnets은 복셀에 적용된 다음 3d 컨볼루션 레이어와 2d 백본 및 감지 헤드에 의해 처리된다. 
이를 통해 엔드 투 엔드 학습이 가능하지만 225ms의 추론 시간이 필요하다. 
second는 더 강력한 성능과 20Hz의 훨씬 향상된 속도를 제공했지만 3d convolution을 제거 할 수 없었다.

1.2 contribution
기둥에 대한 모든 계산이 62Hz에서 추론을 가능하게 하는 조밀한 2d convolution 방법을 보여둔다. 
다른 방법보다 2-4배 빠른 계수이다. 

2. pointpillars network
세가지 주요 단계로 구성된다.
(1) 포인트 클라우드를 sparse pseudo-image로 변환하는 기능 인코더 네트워크
(2) pseudo-image를 높은 수준의 표현으로 처리하기 위한 2d convolution 백본
(3) 3d 상자를 감지하고 회귀시키는 감지 헤드 

2.1. Pointcloud to Pseudo-Image
2d convolution 아키텍쳐를 적용하기 위해 먼저 포인트 클라우드를 pseudo-image로 변환한다.
포인트 클라우드의 변환: 포인트 클라우드의 각 점(l)은 x, y, z 좌표와 반사율(r)을 가지고 있으며, 이를 x-y 평면에서 균일한 그리드로 디스크리트화하여 "기둥(Pillars)"을 만듭니다.
포인트의 확장: 각 기둥 내의 포인트는 x, y, z의 평균 거리(xc, yc, zc)와 기둥 중심에서의 오프셋(xp, yp)으로 확장되어, 최종적으로 9차원 벡터로 인코딩됩니다.
희소성 문제 해결: 포인트 클라우드의 희소성을 해결하기 위해, 각 샘플의 기둥(P)과 기둥 내의 포인트(N) 수에 제한을 두어 밀집된 텐서를 생성합니다. 데이터가 많으면 무작위로 샘플링되고, 부족할 경우 제로 패딩이 적용됩니다.
(라이다 포인트 클라우드에는 ~97% 희소성을 위해 kitti에서 일반적으로 사용되는 범위에서 6k~9k의 비어 있지 않은 기둥이 있다.
이 희소성은 샘플당 비어 있지 않은 기둥의 수와 기둥당 포인트 수에 제한을 두어 조밀한 텐서를 생성한다.)
샘플 또는 기둥에 너무 많은 데이터가 있어 이 텐서에 맞지 않는 경우 데이터가 무작위로 샘플링된다. 
반대로, 샘플 또는 기둥에 데이터가 너무 적어 텐서를 채울 수 없는 경우 0 패딩이 적용된다. 

다음으로, 각 점에 대해 선형 계층이 적용된 다음 batchnorm 및 relu가 적용되어 (c,p,n) 크기의 텐서를 생성하는 단순화된 버전의 pointnet을 사용한다. 
그 다음에는 채널에 대한 max 연산을 수행하여 (c,p) 크기의 출력 텐서를 생성한다. 
선형 계층은 텐서 전체에 걸쳐 1x1 컨볼루션으로 공식확될 수 있으므로 매우 효율적인 계산이 가능하다. 
인코딩이 완료되면 피처는 원래 기둥 위치로 다시 흩어져 크기 (c,h,w)의 psuedo-image를 생성한다. 

2.2. Backbone
백본에는 두 개의 하위 네트워크, 즉 점점 더 작은 공간 해상도로 기능을 생성하는 하향식 네트워크와 하향식 기능의 업샘플링 및 연결을 수행하는 두 번째 네트워크가 있다. 
(Deconv는 Deconvolution 또는 Transposed Convolution의 줄임말)
업샘플링 및 연결: 업샘플링은 Transposed Convolution을 사용하여 원래 이미지 해상도로 복원한다. 그 후 BatchNorm과 ReLU가 적용되며, 마지막으로 다양한 스트라이드에서 생성된 특징들이 연결된다.

2.3 detection head
SSD 설정을 사용하여 3D 물체 감지를 수행한다.
SSD와 마찬가지로 IoU를 사용하여 사전 상자를 실측 자료와 일치시킨다.
경계 상자 높이와 고도는 일치에 사용되지 않는다. 대신 2D 일치가 주어지면 높이와 고도가 추가 회귀 대상이 된다. 

3.1 network
인코더 네트워크에는 c=64 출력 피쳐가 있다.
자동차와 보행자/자전거 백본은 첫 번째 블록의 stride를 제외하고 동일하다. 



