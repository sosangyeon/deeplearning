# Mini-Batch Gradient Descent

## 배치 경사 하강법

배치 경사하강법이란,
전체 학습 데이터를 하나의 배치로 묶어 학습시키는 경사하강법이다.


전체 데이터셋을 모두 계산하기 때문에 메모리가 많이 필요하다.
또한 전체 데이터에 대해 기울기를 구하기 때문에 각 업데이트에서 방향이 일정하고 변하지 않는다.
-> 안정적인 수렴을 할 수 있다. 
이것이 로컬 미니멈에 빠졌을 때, 그곳에서 빠져나오기 어려운 상황을 초래할 수 있다.

이러한 이유들로 SGD, mini-BGD가 나오게 되었다.


## 확률적 경사 하강법

확률적 경사 하강법은 전체 데이터 중 단 하나의 데이터를 이용해 업데이트를 진행하는 방식이다.
하나의 데이터만을 계산하기 때문에 1회 업데이트 속도가 빠르고, '수렴에 shooting'한다는 특징이 있다.
(shooting : 각 데이터에 대한 손실값의 기울기가 다르기 때문에 개별 데이터에 대한 기울기의 방향이 다르다.)

이것이 위에서 언급했던 로컬 미니멈에 빠질 확률을 줄여주는 효과를 가져다준다.
하지만 노이즈가 너무 심하기 때문에 글로벌 로컬에 수렴하기 어렵다는 문제가 있다. 


## 미니 배치 경사 하강법

mini-BGD는 SGD와 BGD의 중간점이다.

전체 데이터셋을 batch-size로 나누어 한 묶음으로 학습시키는 것이다.

shooting이 존재하긴 하지만, 노이즈가 심하지 않아 로컬 미니멈을 피하면서 글로벌 미니멈으로 수렴가능하다.

% 배치사이즈는 2^n의 크기가 좋다고 한다.(GPU 구조)

