Abstract
deeper neural network는 훈련하기가 힘들다. 
깊은 네트워크의 훈련을 용이하게 하기 위해 residual learning 프레임워크를 제시한다.
이러한 네트워크가 최적화하기 더 쉬우며 상당히 증가된 깊이에서 정확도를 얻을 수 있음을 보여준다.
VGG 네트워크보다 8배 더 깊은 최대 152개 레이어 깊이의 잔여 네트워크를 평가하지만 여전히 복잡성은 낮다.

1. Introduction
deep cnn은 이미지 분류를 위한 일련의 돌파구를 마련했다.
딥 네트워크는 자연스럽게 low/mid/high 수준의 특징과 분류자를 엔드 투 엔드 다층 방식으로 통합하며, 특징의 수준은 적층된 계층의 수에 의해 강화될 수 있다.
깊이의 중요성에 힘입어 더 나은 네트워크를 학습하는 것이 더 많은 계층을 쌓는 것만큼 쉬운가에 대한 질문이 생긴다.
문제는 그라디언트의 소멸/폭발이다. 이 문제는 정규화과정을 통해 대부분 해결되었으며, 이를 통해 수십개의 계층을 가진 네트워크가 역전파를 사용해 SGD을 위해 수렴할 수 있다.
네트워크 깊이가 증가함에 따라 정확도는 포화 상태가 되고 급격히 저하된다.
예기치 않게도, 이러한 성능 저하 현상은 과적합으로 인한 것이 아니고, 적절하게 깊은 모델에 더 많은 레이어를 추가하면 더 높은 훈련 오류가 발생한다. 
(네트워크가 너무 깊어지면 표현력 한계와 정보 손실이 발생할 수 있습니다. 각 레이어가 유용한 정보를 제대로 학습하지 못하고, 오히려 더 깊어질수록 신호가 왜곡되거나 희석되면서 훈련 오류가 증가한다.)
(Identity Mapping은 입력을 변경 없이 그대로 출력하는 연산)
얕은 모델에 레이어를 추가한 깊은 아키텍쳐를 고려했을 때, 추가 된 레이어는 Identity Mapping을 진행해 얕은 모델의 학습에 영향을 끼치지 않도록 한다.
하지만 이 방법은 해결책이 될 수 없다.
기본 mapping보다 residual mapping이 최적화하기 더 쉽다는 가설을 세웠다. 
residual learning에서 모델은 입력을 그대로 출력으로 전달하는 Identity Mapping이 최적의 해답이라면, 모델이 복잡한 비선형 레이어를 통해 이를 학습하는 것보다 잔차(residual)를 0으로 만드는 것이 더 쉽다.
F(x)+x의 공식은 'shortcut connection(하나 이상의 레이어를 건너 뛰는 연결)' 피드포워드 신경망에 의해 실현될 수 있다.
이 연결은 ID 매핑을 수행하며, 추가 매개 변수나 계산 복잡성을 추가하지 않는다.

2. Related Work
Residual Representations
이미지 인식에서 VLAD는 딕셔너리에 대한 잔차 벡터로 인코딩하는 표현이다. 
벡터 양자화의 경우 잔여 벡터를 인코딩하는 것이 원래 벡터를 인코딩하는 것보다 더 효과적인 것으로 나타났습니다. 
저수준 비전 및 컴퓨터 그래픽스
편미분 방정식(PDE)을 풀기 위해 멀티그리드(Multigrid) 방법은 문제를 여러 척도의 하위 문제로 재구성하여 각 하위 문제가 잔차를 해결한다.
계층적 기저 전처리는 두 스케일 사이의 잔차 벡터를 사용하여 시스템을 더 빠르게 수렴시키는 대안이다.

Shortcut Connections
MLP를 훈련시키는 초기 방법은 선형 계층을 추가하는 것이다. 
초기 접근: 다층 퍼셉트론(MLP)에서 입력과 출력 사이에 선형 레이어를 추가하는 방식이 있었고, 이 방법은 네트워크의 기울기 소실 및 폭발 문제를 해결하기 위해 몇몇 중간 레이어를 보조 분류기와 직접 연결하는 것이었다.
현대적 접근: "Highway Networks"는 게이트(gating) 함수를 사용해 데이터를 조정하는 방식으로 shortcut connections을 구현했다. 그러나 이러한 게이트가 "닫히면", 잔차 학습이 아닌 비잔차 함수가 학습된다.
잔차 네트워크와 차이점: Residual Networks(ResNets)에서 사용하는 Identity Shortcut은 매개변수 없이 모든 정보를 항상 전달하며, 추가적인 잔차 함수만 학습된다.
결론적으로, ResNet의 Identity Shortcut은 깊은 네트워크에서도 성능을 유지할 수 있는 장점을 가지고 있으며, 이는 깊이 100층 이상의 네트워크에서도 효과를 보여주었다.

3. Deep Residual Learning
3.1. Residual Learning
H(x)를 몇 개의 적층된 레이어에 맞는 기본 매핑으로 간주하고, x는 이러한 레이어 중 첫번째 레이어에 대한 입력을 나타낸다.
잔차 학습에서는 네트워크의 적층된 레이어들이 직접 함수 H(x)를 학습하는 대신, 잔차 함수 F(x) = H(x) - x를 학습하도록 합니다. 여기서 **H(x)**는 네트워크가 학습하려는 원래의 함수, x는 입력값입니다.
네트워크가 단순히 입력을 그대로 전달하는 대신, 입력과 출력의 차이인 잔차(residual)를 학습함으로써 학습 과정이 더 쉬워지고, 특히 깊은 네트워크에서 성능이 개선될 수 있다는 점이다.
ResNet(Residual Network)은 이 접근법을 사용하여 매우 깊은 네트워크에서도 효과적인 학습을 가능하게 한다.
따라서 원래 함수는 F(x) + x가 된다. 두 형식 모두 원하는 함수를 점근적으로 근사할 수 있어야 하지만 학습의 용이성은 다를 수 있다. 

3.2. Identity Mapping by Shortcuts
y = F(x,{W}) + x 으로 정의된 빌딩 블록을 고려한다.
F(x)=W2σ(W1x)를 나타내지만 이 가설은 
비선형 레이어가 충분히 깊어지면 복잡한 함수도 근사할 수 있다는 가정이 널리 받아들여지고 있지만, 그 이론적 근거는 아직 완전히 해명되지 않았다.
y = F(x, {Wi}) + Wsx
ID 매핑이 성능 저하 문제를 해결하기에 충분하고 경제적이므로 Ws는 차원을 일치시킬 때만 사용된다.

3.3 Network Architectures
Plain Network
일반 네트워크, 일반 기준선은 주로 VGG에 영향을 받았다.
컨볼루션 레이어는 대부분 3x3 필터를 가지며 두 가지 간단한 설계 규칙을 따른다.
1) 동일한 출력 피처 맵 크기에 대해 레이어는 동일한 수의 필터를 갖는다.
-> 일관성 유지: 동일한 출력 크기에서는 동일한 수의 필터를 사용함으로써, 네트워크의 각 부분에서 균형 잡힌 처리를 보장한다.
-> 계산 복잡성: 레이어가 동일한 출력 크기를 가지면서 필터 수를 증가시키면 계산 비용이 급격히 증가할 수 있으므로, 필터 수를 동일하게 유지함으로써 계산 복잡성을 일정하게 관리할 수 있다.
2) 기능 맵 크기가 절반으로 줄어들면 레이어 당 시간 복잡성을 유지하기 위해 필터 수가 두 배로 늘어난다.
-> 피처 맵의 크기가 절반으로 줄어들면 각 필터의 계산량이 줄어든다. 따라서 필터 수를 두 배로 늘리면 계산 복잡성을 유지하면서 더 풍부한 특징을 학습할 수 있다.

Residual Network.
직접적인 ID 매핑 (Identity Mapping):
입력과 출력의 차원이 동일하면, 바로가기 연결(Shortcut Connection)을 통해 입력을 그대로 출력으로 전달할 수 있다. 이는 "solid line shortcuts"으로 나타난다.
차원이 증가할 때:
옵션 A: 차원이 증가하면, 0을 채워서 ID 매핑을 유지한다. 이 경우 추가적인 매개변수가 없다.
옵션 B: 1x1 컨볼루션을 사용해 차원을 맞추는 "projection shortcut"을 사용한다.
스트라이드 2:
두 개의 다른 크기의 피처 맵을 연결할 때는 스트라이드 2로 다운샘플링을 수행한다.

