## Neural Discrete Representation Learning

## Abstract
ì´ì‚° í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ê°„ë‹¨í•˜ì§€ë§Œ ê°•ë ¥í•œ ìƒì„± ëª¨ë¸ì„ ì œì•ˆ
VQ-VAEëŠ” ë‘ê°€ì§€ ì£¼ìš” ë©´ì—ì„œ VAEì™€ ë‹¤ë¥´ë‹¤.

(autoregressive decoder : ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ê°•ë ¥í•œ ë””ì½”ë”ì´ë‹¤. 
ì˜ˆë¥¼ ë“¤ì–´, ë¬¸ì¥ì´ë‚˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ë•Œ ì´ì „ì— ìƒì„±ëœ ìš”ì†Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ìš”ì†Œë¥¼ ì˜ˆì¸¡í•˜ê³  ìƒì„±í•œë‹¤.)
autoregressive decoderëŠ” ë§¤ìš° ê°•ë ¥í•˜ê¸° ë•Œë¬¸ì—, ì¸ì½”ë”ê°€ ìƒì„±í•œ ì ì¬ ë³€ìˆ˜ë¥¼ ë¬´ì‹œí•˜ê³ ë„ ì¶©ë¶„íˆ ì¢‹ì€ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤. 
ì´ ê²½ìš° ì¸ì½”ë”ê°€ í•™ìŠµ ê³¼ì •ì—ì„œ ì ì¬ ë³€ìˆ˜ë¥¼ ì œëŒ€ë¡œ í™œìš©í•˜ì§€ ì•Šê²Œ ë˜ì–´, ì ì¬ ë³€ìˆ˜ê°€ ì˜ë¯¸ë¥¼ ìƒì‹¤í•˜ëŠ” 'ì‚¬í›„ ë¶•ê´´' í˜„ìƒì´ ë°œìƒí•œë‹¤.

1. Introduction
ì›ì‹œ ë°ì´í„°(ì˜ˆ: ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ìŒì„± ë“±)ì˜ ì¤‘ìš”í•œ íŠ¹ì§•ë“¤ì„ ì¶”ì¶œí•˜ì—¬ ì´ë¥¼ ê°„ë‹¨í•œ í˜•íƒœë¡œ ë³€í™˜í•œë‹¤. 
ì–´ë ¤ìš´ ì‘ì—…ì„ ì˜ ìˆ˜í–‰í•˜ë ¤ë©´, ë°ì´í„°ë¡œë¶€í„° í•™ìŠµëœ í‘œí˜„ì´ ë§¤ìš° ì¤‘ìš”í•˜ì§€ë§Œ 
ë¹„ì§€ë„ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ëœ í‘œí˜„ì€ ì—¬ì „íˆ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì•¼ì—ì„œ ì£¼ë¥˜ ì ‘ê·¼ ë°©ì‹ì´ ë˜ì§€ ëª»í•˜ê³  ìˆë‹¤.

ìµœëŒ€ ìš°ë„(Maximum Likelihood): ìµœëŒ€ ìš°ë„ëŠ” ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆë„ë¡ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ëŠ” ë°©ë²•
ì˜ˆë¥¼ ë“¤ì–´, ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì´ íŠ¹ì • ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  í™•ë¥ ì´ ìµœëŒ€ê°€ ë˜ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.
ì‰½ê²Œ ë§í•´, ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ë¥¼ ê´€ì°°í•  ê°€ëŠ¥ì„±(likelihood)ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

ì¬êµ¬ì„± ì˜¤ë¥˜(Reconstruction Error): ì¬êµ¬ì„± ì˜¤ë¥˜ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ì–´ì„œ ë‹¤ì‹œ ì›ë˜ ë°ì´í„°ë¡œ ë³µì›í–ˆì„ ë•Œì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒ
ì˜ˆë¥¼ ë“¤ì–´, ì˜¤í† ì¸ì½”ë”(autoencoder) ëª¨ë¸ì€ ì´ë¯¸ì§€ë¥¼ ì••ì¶•í•œ í›„ ë‹¤ì‹œ ë³µì›í•œë‹¤. ì›ë˜ ì´ë¯¸ì§€ì™€ ë³µì›ëœ ì´ë¯¸ì§€ ì‚¬ì´ì˜ ì°¨ì´ê°€ ì¬êµ¬ì„± ì˜¤ë¥˜ì´ë‹¤.
ì¬êµ¬ì„± ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì€ ëª¨ë¸ì´ ì›ë˜ ë°ì´í„°ì˜ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì˜ ìœ ì§€í•˜ë„ë¡ í•˜ëŠ” ëª©í‘œì´ë‹¤.

 ì´ ë°©ë²•ë“¤ì´ ì–¼ë§ˆë‚˜ ìœ ìš©í•œì§€ëŠ” ê·¸ ëª¨ë¸ì´ ì–´ë–¤ ì‘ìš© í”„ë¡œê·¸ë¨ì— ì‚¬ìš©ë˜ëŠ”ì§€ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ì´ë¯¸ì§€ ìƒì„± ì‘ì—…ì—ì„œëŠ” ìµœëŒ€ ìš°ë„ê°€ ì¤‘ìš”í•  ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´, ëª¨ë¸ì´ ìƒì„±í•œ ì´ë¯¸ì§€ê°€ ì‹¤ì œ ë°ì´í„°ì™€ ìµœëŒ€í•œ ìœ ì‚¬í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
ë°˜ë©´ì—, ë°ì´í„° ì••ì¶•ì´ë‚˜ ë…¸ì´ì¦ˆ ì œê±°ì™€ ê°™ì€ ì‘ìš©ì—ì„œëŠ” ì¬êµ¬ì„± ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ë” ì¤‘ìš”í•  ìˆ˜ ìˆë‹¤. ì›ë˜ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ë³µì›í•  ìˆ˜ ìˆëŠ”ì§€ê°€ ê´€ê±´ì´ê¸° ë•Œë¬¸ì´ë‹¤.

-> ëª©í‘œëŠ” ì ì¬ ê³µê°„ì—ì„œ ë°ì´í„°ì˜ ì¤‘ìš”í•œ ê¸°ëŠ¥ì„ ë³´ì¡´í•˜ë©´ì„œ ìµœëŒ€í•œì˜ ê°€ëŠ¥ì„±ì„ ìœ„í•´ ìµœì í™”í•˜ëŠ” ëª¨ë¸ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒ

PixelCNN ê°™ì€ ëª¨ë¸ì€ ì ì¬ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ í›Œë¥­í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ” ìƒì„± ëª¨ë¸ì´ë‹¤. 
í•˜ì§€ë§Œ ë‹¤ì–‘í•œ ì˜ì—¬ê²¡ì„œ ë¶ˆì—°ì†ì ì´ê³  ìœ ìš©í•œ ì ì¬ ë³€ìˆ˜ë¥¼ í•™ìŠµí•´ì•¼ í•œë‹¤. ì¦‰, ì´ì‚° í‘œí˜„ì— ì§‘ì¤‘í•´ì•¼ í•œë‹¤.
ì´ì‚° í‘œí˜„ì€ ë³µì¡í•œ ì¶”ë¡ , ê³„íš ë° ì˜ˆì¸¡ í•™ìŠµì— ìì—°ìŠ¤ëŸ½ê²Œ ì í•©í•˜ë‹¤. 

VQ-VAEëŠ” ì ì¬ ê³µê°„ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— êµ­ì†Œì ì¸ ë…¸ì´ì¦ˆì™€ ì¸ì§€í•  ìˆ˜ ì—†ëŠ” ì„¸ë¶€ ì‚¬í•­ì— ì§‘ì¤‘í•˜ëŠ” ê²ƒê³¼ëŠ”
ëŒ€ì¡°ì ìœ¼ë¡œ ë°ì´í„° ê³µê°„ì˜ ì—¬ëŸ¬ ì°¨ì›ì— ê±¸ì³ ìˆëŠ” ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì„±ê³µì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆë‹¤. 

2. Related Work
discrete latent variablesë¥¼ ì‚¬ìš©í•˜ì—¬ variational autoencodersë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤.
discrete variablesë¥¼ ë”¥ëŸ¬ë‹ì— ì ìš©í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ê²ƒìœ¼ë¡œ ì…ì¦ë˜ì–´ìˆë‹¤. - ê¸°ë³¸ ì–‘ì‹ì´ ë³¸ì§ˆì ìœ¼ë¡œ ë¶ˆì—°ì†ì ì¸ ê²½ìš°ì—ë„

(ë³€ë¶„ ì˜¤í† ì¸ì½”ë”(VAE)ì—ì„œì˜ ëª©í‘œ í•¨ìˆ˜ëŠ” ì£¼ë¡œ ELBOë¥¼ ìµœì í™”í•˜ëŠ” ê²ƒì´ë‹¤. 
single-sample objective : VAEì—ì„œ í•™ìŠµí•  ë•Œ, latent variableì„ í•œ ë²ˆ ìƒ˜í”Œë§í•˜ì—¬ ì´ ìƒ˜í”Œë¡œë¶€í„° ëª©í‘œ í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•
ì´ ë°©ë²•ì€ ê³„ì‚°ì´ ë‹¨ìˆœí•˜ì§€ë§Œ, ìƒ˜í”Œë§ëœ zì— ë”°ë¼ ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤. ì¦‰, ê·¸ë¼ë””ì–¸íŠ¸ì˜ ë³€ë™ì„±(ë¶„ì‚°)ì´ ì»¤ì ¸ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§ˆ ìˆ˜ ìˆë‹¤.
multi-sample objective : ì ì¬ ë³€ìˆ˜ zë¥¼ ì—¬ëŸ¬ ë²ˆ ìƒ˜í”Œë§í•˜ê³ , ê° ìƒ˜í”Œì— ëŒ€í•´ ëª©í‘œ í•¨ìˆ˜ë¥¼ ê³„ì‚°í•œ í›„, ì´ë¥¼ í‰ê· ë‚´ì–´ ìµœì¢… ëª©í‘œ ê°’ì„ ê³„ì‚°í•œë‹¤.)

ì´ì‚°ì  ì ì¬ ë³€ìˆ˜ì™€ í•™ìŠµì˜ ì–´ë ¤ì›€
ì ì¬ ë³€ìˆ˜ê°€ ì´ì‚°ì (discrete)ì¼ ê²½ìš°, ê·¸ë¼ë””ì–¸íŠ¸ ê¸°ë°˜ì˜ ìµœì í™”ê°€ ì–´ë ¤ì›Œì§„ë‹¤. ì´ì‚° ë³€ìˆ˜ëŠ” ê°‘ì‘ìŠ¤ëŸ¬ìš´ ë³€í™”ë¥¼ ê°€ì§€ë©°, ë¯¸ë¶„ ê°€ëŠ¥í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ê¸° ì–´ë µë‹¤.
Continuous Reparameterization : ì¬ë§¤ê°œë³€ìˆ˜í™”ëŠ” ëª¨ë¸ì´ ì´ì‚°ì ì¸ ë¬¸ì œë¥¼ ì—°ì†ì ì¸ ë¬¸ì œë¡œ ë°”ê¿”ì„œ ì²˜ë¦¬í•˜ëŠ” ê¸°ë²•ì´ë‹¤.
Concrete, Gumbel-softmax ë¶„í¬
ê²€ë²¨-ì†Œí”„íŠ¸ë§¥ìŠ¤(Gumbel-softmax)ëŠ” ê²€ë²¨(Gumbel) ë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•œ ê°’ë“¤ì— ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax) í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ì–»ëŠ” ë¶„í¬ì´ë‹¤.
ì´ë¥¼ í†µí•´ ì´ì‚°ì  ì„ íƒ ë¬¸ì œë¥¼ ì—°ì†ì  ê°’ìœ¼ë¡œ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤.
ì½˜í¬ë¦¬íŠ¸ ë¶„í¬ëŠ” ì´ì™€ ìœ ì‚¬í•˜ê²Œ ì´ì‚°ì  ë³€ìˆ˜ë¥¼ ì—°ì†ì ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ í•™ìŠµ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“ ë‹¤.

ì˜¨ë„ ìƒìˆ˜ëŠ” í™•ë¥  ë¶„í¬ì˜ "ë¶€ë“œëŸ¬ì›€"ì„ ì¡°ì ˆí•œë‹¤. ì˜¨ë„ê°€ ë†’ì„ìˆ˜ë¡ ë¶„í¬ëŠ” ë” ë¶€ë“œëŸ¬ì›Œì§€ê³ , ìƒ˜í”Œë“¤ì´ ë” ê· ë“±í•˜ê²Œ ë¶„í¬ëœë‹¤. ë°˜ë©´, ì˜¨ë„ê°€ ë‚®ì•„ì§ˆìˆ˜ë¡ ë¶„í¬ëŠ” ë‚ ì¹´ë¡œì›Œì§€ê³ , íŠ¹ì • ê°’ì— ë” ì§‘ì¤‘ëœë‹¤.
í•™ìŠµ ê³¼ì •ì—ì„œ ì˜¨ë„ ìƒìˆ˜ë¥¼ ì ì°¨ ë‚®ì¶”ëŠ” ê³¼ì •ì„ 'ì–´ë‹ë§(annealing)'ì´ë¼ê³  í•œë‹¤. 

Gaussian reparameterization trickì€ ì—°ì†ì ì¸ ì ì¬ ë³€ìˆ˜ì— ëŒ€í•´ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ í•œë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ê·¸ë¼ë””ì–¸íŠ¸ì˜ ë¶„ì‚°(gradient variance)ì´ ë§¤ìš° ì‘ì•„ì ¸, ë” ì•ˆì •ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.

Scalar QuantizationëŠ” ë°ì´í„°ë¥¼ ì••ì¶•í•˜ëŠ” ê¸°ë³¸ì ì¸ ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤. ì–‘ìí™”ëŠ” ë°ì´í„°ë¥¼ ì¼ì •í•œ ë²”ìœ„ ë‚´ì˜ ê°’ë“¤ë¡œ ê·¼ì‚¬í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ, ì´ ê³¼ì •ì„ í†µí•´ ë°ì´í„°ì˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ ì••ì¶•í•  ìˆ˜ ìˆë‹¤.
ìŠ¤ì¹¼ë¼ ì–‘ìí™”ëŠ” ë°ì´í„°ë¥¼ íŠ¹ì •í•œ ë²”ìœ„ë‚˜ ë‹¨ê³„(step)ë¡œ ë‚˜ëˆ„ê³ , ê° ë°ì´í„°ë¥¼ í•´ë‹¹ ë²”ìœ„ì˜ ëŒ€í‘œ ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤.

Arithmetic Encodingì€ ë°ì´í„°ë¥¼ ë§¤ìš° íš¨ìœ¨ì ìœ¼ë¡œ ì••ì¶•í•  ìˆ˜ ìˆëŠ” ì—”íŠ¸ë¡œí”¼ ë¶€í˜¸í™”(entropy coding) ê¸°ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤.
íŠ¹íˆ ë°ì´í„°ì˜ ë°œìƒ í™•ë¥ ì— ë”°ë¼ ë°ì´í„°ë¥¼ ê°€ë³€ ê¸¸ì´ì˜ ë¹„íŠ¸ì—´ë¡œ ì¸ì½”ë”©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ë°œìƒ í™•ë¥ ì´ ë†’ì€ ë°ì´í„°ëŠ” ì§§ì€ ë¹„íŠ¸ì—´ë¡œ, ë°œìƒ í™•ë¥ ì´ ë‚®ì€ ë°ì´í„°ëŠ” ê¸´ ë¹„íŠ¸ì—´ë¡œ í‘œí˜„ë˜ì–´ ì „ì²´ì ìœ¼ë¡œ ì••ì¶•ì´ ì˜ ëœë‹¤.

Vector Quantization ê³¼ì • 
ì½”ë“œë¶(Codebook) : ë²¡í„° ì–‘ìí™”ì˜ í•µì‹¬ì€ ì½”ë“œë¶ì´ë¼ëŠ” ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ì½”ë“œë¶ì€ ë¯¸ë¦¬ ì •ì˜ëœ ë²¡í„°ë“¤ì˜ ì§‘í•©ìœ¼ë¡œ, ì›ë³¸ ë°ì´í„° ë²¡í„°ë“¤ì„ ê°€ì¥ ì˜ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ” ë²¡í„°ë“¤ì˜ ëª¨ìŒì´ë‹¤.
ì–‘ìí™”(Quantization): ë²¡í„° ì–‘ìí™”ì—ì„œëŠ” ì›ë³¸ ë²¡í„°ë¥¼ ì½”ë“œë¶ì— ìˆëŠ” ê°€ì¥ ê°€ê¹Œìš´ ë²¡í„°ë¡œ ëŒ€ì²´í•œë‹¤. ì´ë¥¼ í†µí•´ ì›ë˜ì˜ ë²¡í„° ê³µê°„ì„ ì½”ë“œë¶ì— ìˆëŠ” ë²¡í„°ë“¤ë¡œ ë¶„í• í•˜ì—¬, ê°ê°ì˜ ì…ë ¥ ë²¡í„°ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ì½”ë“œë¶ ë²¡í„°ë¡œ ë§¤í•‘ë‹¤.
ì••ì¶•(Compression): ë²¡í„° ì–‘ìí™” ê³¼ì •ì„ í†µí•´ ì…ë ¥ ë°ì´í„°ëŠ” ì½”ë“œë¶ì˜ ì¸ë±ìŠ¤ë¡œ ëŒ€ì²´ë˜ë¯€ë¡œ, ë°ì´í„°ê°€ ì••ì¶•ëœë‹¤. ì¦‰, ì›ë˜ ë°ì´í„° ëŒ€ì‹  ì½”ë“œë¶ì—ì„œ ì„ íƒëœ ë²¡í„°ì˜ ì¸ë±ìŠ¤ë§Œ ì €ì¥í•œë‹¤.

hard clustering :  ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ì—ë§Œ ì†í•  ìˆ˜ ìˆë‹¤. ì¦‰, ë°ì´í„° í¬ì¸íŠ¸ê°€ íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹ë˜ë©´, ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì—ëŠ” ì†í•  ìˆ˜ ì—†ë‹¤.
ì–´ë‹ë§(annealing) ê³¼ì •ì€ ë²¡í„° ì–‘ìí™”ì—ì„œ ì‚¬ìš©ëœ ì—°ì†ì  ì´ì™„(continuous relaxation)ì„ ì ì°¨ í•˜ë“œ í´ëŸ¬ìŠ¤í„°ë§(hard clustering)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•œë‹¤.

ì˜¤í† ì¸ì½”ë”ì™€ ë²¡í„° ì–‘ìí™”ë¥¼ ê²°í•©í•˜ì—¬ ì‹ ê²½ë§ì„ í›ˆë ¨í•˜ëŠ” ë°©ë²•
1. ì˜¤í† ì¸ì½”ë” í•™ìŠµ
2. ë²¡í„° ì–‘ìí™”(Vector Quantization) ì ìš© - ì¸ì½”ë”ì˜ ì¶œë ¥ì„ ì‚¬ì „ì— ì •ì˜ëœ ì½”ë“œë¶ì˜ ë²¡í„°ë“¤ë¡œ ì–‘ìí™”í•˜ëŠ” ê³¼ì •
3. Soft-to-Hard Relaxation - ë„¤íŠ¸ì›Œí¬ ì „ì²´ë¥¼ ì‘ì€ í•™ìŠµë¥ ë¡œ ë‹¤ì‹œ í›ˆë ¨ì‹œì¼œ, ì†Œí”„íŠ¸ì—ì„œ í•˜ë“œë¡œ ì „í™˜í•˜ëŠ” ê³¼ì •ì„ ì•ˆì •í™”

ì‹¤í—˜ì—ì„œì˜ ë¬¸ì œ:
ì‹¤í—˜ ê³¼ì •ì—ì„œ ì €ìë“¤ì´ ë°œê²¬í•œ ë¬¸ì œëŠ” ë””ì½”ë”ê°€ ì—°ì†ì ì¸ ì´ì™„(continuous relaxation)ì„ ì—­ì „ì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ì—ˆë‹¤.
(ì¦‰, ì–‘ìí™”ê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì•˜ê³ , ë””ì½”ë”ê°€ ì—°ì†ì ì¸ ê°’ìœ¼ë¡œ ëŒì•„ê°€ ë²„ë ¸ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ ì›í•˜ëŠ” ëŒ€ë¡œ ì´ì‚°ì  ë°ì´í„°ë¥¼ ë§Œë“¤ì§€ ëª»í–ˆë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.)

3. VQ-VAE
VAEëŠ” ì…ë ¥ ë°ì´í„° xê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì´ì‚° ì ì¬ í™•ë¥  ë³€ìˆ˜ zì˜ ì‚¬í›„ ë¶„í¬ q(z|x)ë¥¼ ë§¤ê°œë³€ìˆ˜í™”í•˜ëŠ” ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬, ì‚¬ì „ ë¶„í¬ p(z), ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ ë¶„í¬ p(x|z)ë¥¼ ê°–ëŠ” ë””ì½”ë”ë¡œ êµ¬ì„±ëœë‹¤.
VAEì˜ ì‚¬í›„ ë° ì‚¬ì „ ë¶„í¬ëŠ” ëŒ€ê°ì„  ê³µë¶„ì‚°ìœ¼ë¡œ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •ë˜ë©°, ì´ë¥¼ í†µí•´ ê°€ìš°ìŠ¤ ì¬ë§¤ê°œë³€ìˆ˜í™” ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
VQ_VAEì—ì„œëŠ” ì‚¬í›„ ë¶„í¬ì™€ ì‚¬ì „ ë¶„í¬ëŠ” categorical(ì´ì‚°ì )ì´ë©°, ì´ëŸ¬í•œ ë¶„í¬ì—ì„œ ì¶”ì¶œí•œ í‘œë³¸ì€ ì„ë² ë”© í…Œì´ë¸”ì„ ì¸ë±ì‹±í•œë‹¤.
(Posterior Distributionì—ì„œ ë²”ì£¼í˜• ê°’ì„ ìƒ˜í”Œë§í•œë‹¤. ìƒ˜í”Œë§ëœ ì´ ë²”ì£¼í˜• ê°’ì€ ì„ë² ë”© í…Œì´ë¸”ì—ì„œ íŠ¹ì • ìœ„ì¹˜ë¥¼ ê°€ë¦¬í‚¤ëŠ” ì¸ë±ìŠ¤ ì—­í• í•œë‹¤. 
ì„ë² ë”© í…Œì´ë¸”ì€ ê° ë²”ì£¼ì— ëŒ€í•œ ê³ ì°¨ì› ë²¡í„°(ì„ë² ë”©)ë¥¼ ì €ì¥í•œ ë°ì´í„°ë² ì´ìŠ¤ì´ë‹¤.)

3.1 Discrete Latent variables
eâˆˆR{KÃ—D} ì ì¬ ì„ë² ë”© ê³µê°„ ì •ì˜
KëŠ” ì´ì‚° ì ì¬ ê³µê°„(K-way ë²”ì£¼í˜•)ì˜ í¬ê¸°ì´ê³  DëŠ” ê° ì ì¬ ì„ë² ë”© ë²¡í„° eiì˜ ì°¨ì›ì´ë‹¤.

ì…ë ¥ xë¥¼ ì¸ì½”ë”ì— í†µê³¼ì‹œí‚¤ë©´ z_e(x)ë¼ëŠ” ì¶œë ¥ì´ ìƒì„±ëœë‹¤. ì´ëŠ” ì—°ì†ì ì¸ ê°’ìœ¼ë¡œ í‘œí˜„ëœ ë²¡í„°ì´ë‹¤.
ìƒì„±ëœ ze(x)ì™€ ì„ë² ë”© ê³µê°„ì— ìˆëŠ” Kê°œì˜ ë²¡í„°ë“¤ ê°„ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ì—¬, ê°€ì¥ ê°€ê¹Œìš´ ë²¡í„° ekë¥¼ ì°¾ëŠ”ë‹¤. 
ì´ ë²¡í„°ê°€ ì´ì‚°ì ì¸ ì ì¬ ë³€ìˆ˜ zì— í•´ë‹¹í•œë‹¤.
ì°¾ì•„ë‚¸ ì„ë² ë”© ë²¡í„° ğ‘’ğ‘˜ëŠ” ë””ì½”ë”ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë˜ì–´ ë°ì´í„°ë¥¼ ë³µì›í•˜ëŠ” ë° í™œìš©ëœë‹¤.

3.2 Learning
ì–‘ìí™”ëŠ” ì´ì‚°ì ì¸ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ë¯€ë¡œ, ì´ ê³¼ì •ì—ì„œ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì–´ë ¤ìš´ ë¬¸ì œê°€ ëœë‹¤.
Straight-through estimator : ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê·¼ì‚¬í•˜ì—¬, ë””ì½”ë” ì…ë ¥ì—ì„œ ì¸ì½”ë” ì¶œë ¥ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ë‹¨ìˆœíˆ ë³µì‚¬í•˜ëŠ” ë°©ì‹
ì¦‰, ì–‘ìí™” ë‹¨ê³„ì—ì„œ ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ, ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ìš°íšŒí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

ì—­ë°©í–¥ ê³„ì‚°(ê·¸ë˜ë””ì–¸íŠ¸ ì „íŒŒ) : ë””ì½”ë”ì˜ ì¶œë ¥ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ì¸ì½”ë”ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬ëœë‹¤.
ì¸ì½”ë” ì¶œë ¥ ze(x)ì™€ ë””ì½”ë” ì…ë ¥ zq(x)ê°€ ê°™ì€ ì°¨ì› ê³µê°„ì—ì„œ í‘œí˜„ë˜ë¯€ë¡œ, ì¬êµ¬ì„± ì†ì‹¤ì„ ì¤„ì´ê¸° ìœ„í•´ ì¸ì½”ë”ê°€ ì–´ë–»ê²Œ ì¶œë ¥ì„ ì¡°ì •í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ê·¸ë˜ë””ì–¸íŠ¸ê°€ í¬í•¨í•˜ê³  ìˆë‹¤.

VQ-VAEì˜ ì„¸ê°€ì§€ êµ¬ì„± ìš”ì†Œ
ì¬êµ¬ì„± ì†ì‹¤: ë””ì½”ë”ê°€ ì›ë³¸ ì…ë ¥ì„ ì–¼ë§ˆë‚˜ ì˜ ë³µì›í•˜ëŠ”ì§€ë¥¼ í‰ê°€í•˜ë©°, ì¸ì½”ë”ì™€ ë””ì½”ë” ëª¨ë‘ ìµœì í™”ëœë‹¤.
ì„ë² ë”© ì†ì‹¤: ì„ë² ë”© ë²¡í„°ë¥¼ ì¸ì½”ë” ì¶œë ¥ì— ê°€ê¹ê²Œ ì´ë™ì‹œí‚¤ë©°, ì„ë² ë”©ë§Œ ìµœì í™”ëœë‹¤.
ì»¤ë°‹ë¨¼íŠ¸ ì†ì‹¤: ì¸ì½”ë”ê°€ ì„ë² ë”© ë²¡í„°ì— ë” ì˜ ë§ì¶”ë„ë¡ ìœ ë„í•œë‹¤.

L = log p(x|zq(x)) + ksg[ze(x)] âˆ’ ek22 + Î²kze(x) âˆ’ sg[e]k22 (sg : stop-gradient)

ì¬êµ¬ì„± ì†ì‹¤
ì¬êµ¬ì„± ì†ì‹¤í•­ì—ì„œ Straight-through gradient estimationìœ¼ë¡œ ì„ë² ë”© eiê°€ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ë°›ì§€ ì•ŠëŠ”ë‹¤.
ë”°ë¼ì„œ ì„ë² ë”© ê³µê°„ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ê°€ì¥ ê°„ë‹¨í•œ ì‚¬ì „ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì¸ VQ(Vector Quantisation)ë¥¼ ì‚¬ìš©í•œë‹¤.

ì„ë² ë”© ì†ì‹¤
ì„ë² ë”© ë²¡í„°eië¥¼ ì¸ì½”ë” ì¶œë ¥ ze(x) ìª½ìœ¼ë¡œ ì´ë™

ì»¤ë°‹ë¨¼íŠ¸ ì†ì‹¤
ë§Œì•½ ì¸ì½”ë”ê°€ ì„ë² ë”© ë²¡í„°ì— ì¶©ë¶„íˆ ë¹ ë¥´ê²Œ ë§ì¶°ì§€ì§€ ì•Šìœ¼ë©´, ì¸ì½”ë”ê°€ ì¶œë ¥í•˜ëŠ” ê°’ì´ ê³„ì† ì»¤ì ¸ì„œ ì„ë² ë”© ë²¡í„°ì™€ì˜ ê±°ë¦¬ê°€ ì»¤ì§ˆ ìˆ˜ ìˆë‹¤.
ì´ë¡œ ì¸í•´, ì„ë² ë”© ê³µê°„ì´ ê³„ì† í™•ì¥ë˜ì–´ ëª¨ë¸ì´ ë¶ˆì•ˆì •í•´ì§ˆ ìˆ˜ ìˆë‹¤.

ë””ì½”ë”ëŠ” ì¬êµ¬ì„± ì†ì‹¤í•­ì—, ì¸ì½”ë”ëŠ” ì¬êµ¬ì„± ì†ì‹¤, ì»¤ë°‹ë¨¼íŠ¸ ì†ì‹¤ì— ì˜í•´ ìµœì í™”ë˜ë©°, ì„ë² ë”©ì€ ì„ë² ë”© ì†ì‹¤ì— ì˜í•´ ìµœì í™”ëœë‹¤.

Î²ëŠ” ì»¤ë°‹ë¨¼íŠ¸ ì†ì‹¤ì˜ ê°€ì¤‘ì¹˜ì´ë‹¤. Î² ê°’ì´ 0.1ì—ì„œ 2.0 ì‚¬ì´ì—ì„œ ê²°ê³¼ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, Î² ê°’ì— ëŒ€í•´ ì•Œê³ ë¦¬ì¦˜ì´ ê°•ê±´í•˜ë‹¤ëŠ” ê²°ë¡ ì„ ì–»ì—ˆë‹¤. ëª¨ë“  ì‹¤í—˜ì—ì„œëŠ” Î² = 0.25ë¥¼ ì‚¬ìš©í–ˆë‹¤.

ë””ì½”ë”ê°€ ì™„ì „íˆ ìˆ˜ë ´í•œ í›„ì—ëŠ”, 
zê°€ 

zq(x)ì™€ ë‹¤ë¥¼ ë•Œ p(xâˆ£z)ì— í™•ë¥ ì„ í• ë‹¹í•˜ì§€ ì•Šì•„ì•¼ í•œë‹¤. ì¦‰, ë””ì½”ë”ëŠ” zq(x)ë¡œë§Œ ë°ì´í„°ë¥¼ ì¬êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµëœë‹¤.
-> log p(x) â‰ˆ log p(x|zq(x))p(zq(x)).

Jensen's Inequalityì— ë”°ë¼ log p(x) â‰¥ log p(x|zq(x))p(zq(x)).
