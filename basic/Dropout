# Dropout

## Dropout: A Simple Way to Prevent Neural Networks from Overfitting 논문 리뷰

## Abstract
문제 
많은 수의 매개변수가 있는 심층 신경망에서 과적합은 심각한 문제이다.
이를 해결하기 위해 여러 모델을 결합하여 과적합을 처리하는데 이는 비용이 많이 든다.

드롭아웃은 이 문제를 해결한다.

핵심 아이디어 - 훈련 중에 신경망에서 단위를 무작위로 끊어내는 것이다.
학습시에는, 많은 수의 서로 다른 'thinned' 네트워크를 추출한다. (드롭아웃 샘플)
테스트시에는, 단일 신경망을 사용해 예측을 하는데, 작아진 가중치를 사용하여 모든 얇아진 신경망의 예측값을 평균화하는 
효과를 쉽게 근사화할 수 있다.

-> 이런 방식을 통해 과적합을 방지할 수 있다.

## Introduction
심층 신경망(DNN)은 입력-출력간의 복잡한 관계를 학습할 수 있는 모델이다.
학습데이터는 실제 테스트에는 존재하지 않는 노이즈 등이 있을 수 있고, 학습 데이터에 과적합할 수 있다.

과적합 방지 방법 
성능 악화시 훈련 중단, L1 및 L2 정규화 및 소프트 웨이트 공유

가장 좋은 모델 성능 일반화 방법
매개 변수의 가능한 모든 설정에 대한 예측을 계산하고, 예측값들의 평균을 구하는 것이다.
( Bayesian gold standard )

모델 조합은 기계 학습 방법의 성능을 향상시킨다. 
하지만 이는 계산량이 매우 많고, 각 아키텍처에 대한 최적의 하이퍼파라미터를 찾는 것은 어려운 작업이다. 
또한 다양한 네트워크를 테스트 시간에 모두 사용하는 것은 신속한 응답을 필요로 하는 애플리케이션에서 불가능하다.

드롭아웃은 위 문제를 해결한다.

-신경망에서 얇은 네트워크들을 샘플링하는 것과 같은 효과를 가진다. 
n개 단위를 가진 신경망은 2^n개의 얇은 신경망들의 모음으로 볼 수 있다.
이 신경망들은 모두 가중치를 공유하므로 총 매개변수는 동일하다. 

가중치에 p*W 확률을 곱해줌으로써 평균 예측과 근사화된 결과를 얻을 수 있다.

## Motivation
드롭아웃(dropout)의 동기
진화에서 성적 생식의 역할과 비슷한 개념으로 접근한다. 
성적 생식은 부모의 절반씩 유전자를 결합해 새로운 개체를 만든다. 
이는 유전자의 혼합 가능성을 높여 유전자 세트가 다른 유전자와 잘 작동하도록 만든다. 

이는 신경망에서 드롭아웃이 각 은닉 유닛이 다른 유닛 없이도 유용한 기능을 학습하게 만드는 것과 유사합니다.
이로 인해 모델이 더 강력해지고 과적합을 줄일 수 있습니다.



