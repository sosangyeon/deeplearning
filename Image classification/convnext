Abstract
이미지 분류 모델로서 convnets를 대체한 vit는 객체 감지 및 의미론적 분할과 같은 일반적인 cv 작업에 적용할 때 어려움에 직면한다.
swin transformer는 여러 convnet 이전 버전을 재도입하여 트랜스포머를 일반 비전 백본으로 실용적으로 만들고 다양한 비전 작업에서 놀라운 성능을 보여주었다.
그러나 이러한 하이브리드 접근 방식의 효과는 여전히 컨볼루션의 고유한 귀납적 편향보다는 트랜스포머의 본직적인 우월성에 크게 기인한다.
이 작업에서는 디자인 공간을 재검토하고 순수 convnet이 달설할 수 있는 한계를 테스트한다. 
비전 트랜스포머 설계를 위해 표준 resnet을 점진적으로 '현대화'하고 그 과정에서 성능 차이에 기여하는 몇 가지 주요 구성 요소를 발견한다. 
표준 convnet 모듈로만 구성된 convnexs는 표준 convnet의 단순성과 효율성을 유지하면서 87.8%의 imagenet top-1 정확도를 달성하고 swin transformer를 능가하는 정확도와 확장성 측면에서 transformer와 경쟁한다.

1. Introduction
2010년대를 돌이켜보면, 컨볼루션 신경망(convnets)의 르네상스였다.
AlexNet의 도입은 컴퓨터 비전의 새로운 시대를 열었다. 
VGGNet, Inceptions, ResNe(x)t, Densenet, MobileNet, efficientnet 및 regnet 과 같은 대표적인 convnet은 정확성, 효율성 및 확장성의 다양한 측면에 중점을 두고 많은 유용한 설계 원칙을 대중화했다.
많은 애플리케이션 시나리오에서 '슬라이딩 윈도우'전략은 특히 고해상도 이미지로 작업할 때 시각적 처리에 내재되어 있다.
convnet에는 여러 가지 inductive bias가 내장되어 있어 다양한 컴퓨터 비전 애플리케이션에 적합하다.
가장 중요한 것은 translation equivariance로, 감지와 같은 작업에 바람직한 속성이다. 
convnet은 슬라이딩 윈도우 방식으로 사용될 때 계산이 공유된다는 사실 때문에 본질적으로 효율적이다. 

2010년대부터 region-based detector는 convnet을 시각적 인식 시스템의 기본 빌딩 블록으로 더욱 끌어올렸다. 
비슷한 시기에 자연어 처리를 위한 신경망 설계는 트랜스포머가 순환 신경망을 대체하여 지배적인 백본 아키텍처가 되면서 매우 다른 경로를 택했다.
언어와 시각 도메인 간의 관심 과제의 차이에도 불구하고, 비전 트랜스포머의 도입으로 네트워크 아키텍처 설계의 환경이 완전히 바뀌면서 두 흐름이 놀랍게 수렴되었다.
이미지 일려의 패치로 분할하는 초기 'patchify' 계층을 제외하고 vit는 이미지별 inductive bias를 도입하지 않으며 원래 nlp 트랜스포머에 최소한의 변경을 가한다. 
vit의 주요 초점 중 하나는 확장 동작에 있다. 
더 큰 모델 및 데이터 세트 크기의 도움으로 트랜스포머는 표준 resnet을 상당한 차이로 능가할 수 있다.
이미지 분류 작업에 대한 이러한 결과는 고무적이지만, 컴퓨터 비전은 이미지 분류에만 국한되지 않는다.
앞서 논의한 바와 같이, 지난 10년 동안 수많은 컴퓨터 비전 작업에 대한 슬라이딩 윈도우, 완전 컨볼루션 패러다임에 크게 의존했다.
이러한 convnet inductive bias가 없으면 vit는 일반 비전 백본으로 채택되는데 많은 어려움에 직면한다.
가장 큰 과제는 입력 크기와 관련하여 2차 복잡성을 갖는 vit의 글로벌 어텐션 설계이다. 이는 imagenet 분류에 적합할 수 있지만, 고해상도 입력값에서는 금방 다루기 어려워진다.

Hierarchical Transformers는 이러한 격차를 해소하기 위해 하이브리드 접근 방식을 사용한다.
예를 들어, 슬라이딩 윈도우 전략이 트랜스포머에 다시 도입되어 convnet과 더 유사하게 동작할 수 있게 되었다. 
swin transformer는 이러한 방향의 이정표가 되는 작업으로, 트랜스포머가 일반적인 비전 백본으로 채택될 수 있고 이미지 분류를 넘어 다양한 컴퓨터 비전 작업에서 최첨단 성능을 달성할 수 있음을 처음으로 보여주었다.
이러한 관점에서 컴퓨터 비전을 위한 트랜스포머의 많은 발전은 컨볼루션을 되살리는 것을 목표로 했다. 
그러나 이러한 시도에는 대가가 따른다. 슬라이딩 윈도우 self-attention의 순수한 구현은 비용이 많이 들 수 있다. 
다른 한편으로는, convnet이 비록 간단하고 군더더기 없는 방식이기는 하지만, 이미 원하는 속서들 중 많은 부분을 만족하고 있다는 것은 거의 아이러니한 일이다.
convnets이 힘을 잃고 있는 것처럼 보이는 유일한 이유는 트랜스포머가 많은 비전 작업에서 그들을 능가하기 때문이다. 
성능 차이는 일반적으로 멀티 헤드 self-attention이 핵심 구성 요소인 트랜스포머의 우수한 스케일링 동작에 기인한다.

convnet과 Hierarchical Vision Transformers는 서로 다르면서 동시에 유사해진다. 
둘 다 유사한 inductive bias를 갖추고 있지만, 훈련 절차와 거시적/미시적 수준의 아키텍처 설계에서 크게 다르다. 
우리는 점진적으로 아키텍처를 '현대화'하여 계층적 swin-T를 구축한다. 
이 논문의 핵심 질문, 즉 트랜스포머의 설계 결정이 convnets의 성능에 어떤 영향을 미치는가에 대한 질문으로 이루어진다.

2. Modernizing a ConvNet: a Roadmap
resnet에서 transformer와 유사한 convnet으로 이동하는 궤적을 제공한다.
FLOPs의 관점에서 두 가지 모델 크기를 고려하는데, 하나는 FLOP가 약 4.5x109인 ResNet-50/Sin-R체제이고 다른 하나는 FLOPrk 약 15.0x190인 resnet-200/swin-B체제이다. 
1) 매크로 디자인
2) ResNext
3) 역병목현상
4) 큰 커널 크기 
5) 다양한 계층별 마이크로 디자인
네트워크 복잡성은 최종 성능과 밀접한 상관 관계가 있기 때문에 FLOP는 탐색 과정에서 대략적으로 제어되지만 중간 단곙에서는
FLOP가 참조 모델보다 높거나 낮을 수 있다. 

2.1 Training Techniques
네트워크 아키텍처의 설계 외에도 훈련 절차는 최종 성능에도 영향을 미친다.
비전 트랜스포머는 새로운 모듈 세트와 아키텍처 설계 결정을 가져왔을 뿐만 아니라 비전에 다양한 훈련 기술을 도입했다.
이는 주로 최적화 전략 및 관련 하이퍼 매개변수 설정과 관련이 있다.
훈련은 resnet의 원래 90epoch에서 300epoch로 확장된다.
우리는 AdamW optimizer, Mixup, cutmix,randaugment, random erasing과 같은 데이터 증강 기술, stochastic depth 및 label smoothing을 포함한 정규화 체계를 사용한다.
vit와 동일하게 adamW optimizer를 사용
DeiT와 비슷한 하이퍼 파라미터 사용
mixuo, cutmix, randaugment 등의 data augmentation 사용
이 향상된 훈련 레시피는 resnet50모델의 성능을 76.1%에서 78.8%로 향상시켰으며, 이는 기존 convnet과 비전 트랜스포머 간의 성능 차이의 상당 부분이 훈련 기술에 기인할 수 있음을 의미한다. 

2.2 Marco Design
swin transformer는 convnet을 따라 각 스테이지가 서로 다른 기능 맵 해상도를 갖는 다단계 설계를 사용한다. 
두 가지 흥미로운 설계 고려 사항이 있다. 
스테이지 계산 비율과 'stem cell'구조이다.

changing stage compute ratio
resnet의 스테이지 간 계산 분포의 원래 설계는 대체로 경험적이었다.
무거운 'res4'단계는 검출기 헤드가 14x14 형상 평면에서 작동하는 물체 감지와 같은 다운스트림 작업과 호환되도록 고안되었다.
반면 swin-T는 동일한 원칙을 따랐지만 1:1:3:1의 약간 다른 스테이지 컴퓨팅 비율을 따랐다.
더 큰 swin transformer의 경우 비율은 1:1:9:1이다.
설계에 따라 각 단계의 블록 수를 resnet50의 (3.4.6.3)dptj (3,3,9,3)으로 조정하며, 이는 FLOP을 swin-T에 맞춘다. 
이렇게 하면 모델 정확도가 78.8%에서 79.4%로 향상된다. 

changing stem to 'Patchify'
swin transformer에서 patchify는 resnet의 첫 conv레이어와 비슷하다
resnet의 첫 conv 레이어의 kernel 크기와 stride를 각각 7->4, 2->4 로 조정하여 swin transformer와 비슷하게 함.

2.3 resnext-ify
resnet보다 flop/정확도 트레이드오프가 더 나은 resnext의 아이디어를 채택하려고 한다.
핵심 구성 요소는 그룹화된 컨볼루션이며, 여기서 컨볼루션 필터는 서로 다른 그룹으로 분리된다.
높은 수준에서 resnext의 기본 원칙은 '더 많은 그룹을 사용하고 너비를 확장'하는 것이다.
이렇게 하면 flop가 크게 줄어들기 때문에 용량 손실을 보상하기 위해 네트워크 너비가 확장된다.
변경 이유
resnext의 group convolution이 flops를 크게 줄이면서 accuracy는 최대한 유지할 수 있음
depthwise convolution이 self-attention 구조에서 가중합 연산과 비슷하다고 판단(한 차원의 내에서만 정보를 혼합함)
depthwise conv 이후 1x1 conv가 depthwise의 채널별 정보가 섞이도록 함
변경점
3x3 conv를 3x3 depthwise conv로 대체
연산량의 감소로 채널의 너비를 증가시킴

2.4 inverted bottelneck
transformer의 mlp 블록이 중간에서 채널을 4배로 늘렸다 줄이는 inverted bottleneck 구조를 사용
inverted bottleneck 구조를 사용하면 shorcut 1x1 conv레이어에서 채널의 수가 감소하기 때문에 전체 flops가 오히려 줄어듦

2.5 Large Kernel Sizes
변경 이유
Swin Transformer의 Window 크기는 7×7임
Conv 의 Kernel 크기는 성능에 중요한 영향을 미침
변경점
3×3 크기의 Kernel을 7×7 크기로 변경 ( 3, 5, 7, 9, 11 ... 중 제일 좋은 성능을 보임)

2.6 Micro Design
Replacing ReLU with GELU (80.6% -> 80.6%)
변경 이유

Swin Transformer은 ReLU 대신 GELU를 사용함
변경점
비선형 함수를 ReLU 대신 GELU를 사용함
Fewer normalization layers ( 80.6% -> 81.3% )
변경 이유
Transformer에서 비선형 함수는 MLP에 존재하는데 MLP내에 비선형 함수는 하나만 존재함
변경점
두 1×1 Conv 사이에만 비선형 변환 GELU 사용
Substituting BN with LN ( 81.3% -> 81.4%)
변경 이유
Transformer에서는 주로 Layer Normalization을 사용함
변경점
Batch Normalization을 Layer Normalization으로 변경
Separate downsampling layers ( 81.4% -> 82.0% )
변경 이유
Swin Transformer는 다음 계층으로 갈 때 인접한 윈도우를 하나로 뭉치며 이를 CNN에서는 Downsampling 레이어라고 판단
변경점
ConvNeXt의 Downsampling 레이어에서 3×3 Conv를 2×2로 변경함

최종적으로 Swin-T의 성능인 81.3%를 능가하는 82.0%의 성능을 달성했다.
실험했던 모델 구조들은 이미 ConvNet에 대한 이전의 연구들에서 나온 것이다.
단지 적절히 조합하여 순수한 ConvNet의 구조를 통해 당시의 ViT 모델들보다 좋은 성능을 냈다는 점에서 의의가 있다.

