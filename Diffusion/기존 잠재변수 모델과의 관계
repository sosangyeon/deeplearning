확산 모델은 인식 모델을 학습하지 않고 고정 확산 과정을 이용한다. 
기존의 VAE와 같은 잠재 변수 모델은 인식 모델과 생성 모델을 동시에 학습했다.

일반적으로 사후분포는 우도보다 복잡해서 인식 모델의 학습은 어렵다.
이것은 생성 과정이 희소한 인과관계를 바탕으로 구성되어 있지만 사후확률분포는 반드시 그렇지만은 않기 때문이다.
그래서 VAE에서는 생성 모델보다 인식 모델 쪽에서 훨씬 강력한 모델을 사용해야 한다. 또한 학습 진행에 있어서 생성 모델의 변화에 맞춰 인식 모델도 동시에 변화해야 한다는 점도 학습을 어렵게 한다. 

확산 모델은 여러 개의 확률층을 사용한 모델로 표현력을 매우 크게 할 수 있다. 
확률층이 많은 모델을 직접 오차 역전파법으로 학습하려면 중간층 상태를 기억해야 하므로 계산량이나 메모리 사용량이 많아지는 문제가 있다.

하지만 확산 과정은 임의의 깊이의 상태를 해석적으로 복원할 수 있고 또 목적함수가 시간별 디노이징의 합으로 독립이다. 
따라서 확산 모델에서는 중간 확률층을 추출해서 그 층에 대한 회귀 문제를 사용해서 학습할 수 있으므로 효율적으로 학습할 수 있다. 
또 확산 모델은 모든 확률층이 같은 모델을 공유하고 있기 때문에 매개변수의 수는 확률층의 개수에 의존하지 않고 일정하다.

확산 모델은 오차 역전파법을 사용해서 학습할 수 없는 매우 큰 생성 과정을 가진 모델을 학습시킬 수 있으며 생성 능력을 대폭 개선했다고 할 수 있다. 


## 확산 모델 안정적

확산 모델은 기존의 생성 모데로다 학습이 안정적이다. -> 하나의 모델을 학습하는 것만으로 다양한 작업을 수행

기존의 생성 모델은 학습이 불안정해지기 쉬운데, 여러 모델을 학습해야하기 때문이다. 
예를 들어 GAN은 생성기와 분류기 두 개를 경쟁시키면서 학습하기 때문에 학습 도중에 분류기가 강해지기 쉬우며 학습은 불안정해진다. 
또 VAE에서는 인식기와 생성기 2개를 학습시키야 한다. 

확산 모델에서 인식기는 고정, 확산 과정을 이용하면 생성 과정에 대한 모델을 학습.
-> 학습이 안정적이기 때문에 기존보다 훨씬 큰 모델과 대량의 훈련 데이터로 학습시킬 수 있다. 
