Abstract
기존의 자율 주행 데이터 세트는 캡처하는 환경의 규모와 변형이 제한되어 있지만, 운영 지역 내 및 지역 간의 일반화가 기술의 전반적인 실행 가능성에 매우 중요하다.
새로운 데이터 세트는 각각 20초에 걸쳐 있는 1,150개의 장면으로 구성되며, 다양한 도시 및 교외 지역에서 캡처된 잘 동기화되고 보정된 고품질 LiDAR 및 카메라 데이터로 구성된다.
2D 및 3D 감지 및 추적 작업을 위한 강력한 기준을 제공한다.
또한 데이터 세트 크기와 지역 간 일반화가 3D 감지 방법에 미치는 영향을 연구한다.

1. Introduction
자율 주행 기술은 로보택시에서 자율 주행 트럭에 이르기까지 많은 사람의 생명을 구할 수 있는 잠재력을 가진 광범위한 애플리케이션을 가능하게 할 것으로 예상된다.
우리 데이터 세트에서 캡처한 지리적 영역은 절대 영역 범위와 지역 간 해당 범위의 분포 측면에서 다른 유사한 자율 주행 데이터 세트가 다루는 영역보다 훨씬 크다.
여러 도시의 다양한 조건에 걸쳐 기록되었으며 각 도시 내의 넓은 지리적 범위를 가지고 있다.
우리는 이러한 지리적 위치의 차이로 인해 도메인 격차가 뚜렷하게 발생하여 도메인 적응 분야에서 흥미로운 연구 기회를 제공한다는 것을 보여준다.
모든 지상 실측 상자에는 객체 추적을 지원하는 추적 식별자가 포함되어 있다.
연구자들은 우리가 제공한 shutter aware projection library를 사용하여 3D LiDAR 상자에서 2D amodal 카메라 상자를 추출할 수 있다.
(Shutter aware projection library: 
이 라이브러리는 LiDAR의 3D 정보를 카메라의 2D 이미지 평면으로 정확히 변환하는 도구이다. 
이는 카메라의 셔터 동작이 시간이 흐르면서 이미지를 캡처하기 때문에, 3D 데이터를 2D로 변환할 때 이 시간 차이를 고려하여 더 정밀한 결과를 얻도록 도와준다.)
(2D amodal 카메라 상자: 
카메라 이미지에서 객체의 경계선을 표시하는 2차원 상자를 의미한다. 
"Amodal"은 객체가 카메라의 시야에서 부분적으로 가려져 있거나 보이지 않더라도 전체 객체를 추정하는 것을 의미한다.)
멀티모달 실측 자료는 LiDAR와 카메라 주석을 모두 활용하는 센서 융합 연구를 촉진한다.
데이터 세트에는 약 1,200만 개의 카메라 상자 주석이 포함되어 있어 약 113k LiDAR 개체 트랙과 약 250k 카메라 이미지 트랙이 생성된다.
카메라와 LiDAR 판독값 간의 동기화를 제공하여 도메인 간 학습 및 전송을 위한 흥미로운 기회를 제공한다.
연신율과 같은 센서 기능 외에도 각 범위의 이미지 픽셀에 정확한 차량 포즈를 제공한다.
이것은 낮은 수준의 동기화된 정보를 사용할 수 있는 최초의 데이터 세트로, 널리 사용되는 3D 포인트 세트 형식 이외의 LiDAR 입력 표현에 대한 연구를 더 쉽게 수행할 수 있다.
현재 데이터 세트는 교육 및 검증을 위한 1,000개의 장면과 테스트용 150개의 장면으로 구성되어 있으며, 각 장면은 20초에 걸쳐 있다.
지리적 홀드아웃 영역에서 테스트 세트 장면을 선택하면 데이터 세트에서 훈련된 모델이 이전에 못한 영역에서 테스트 세트 장면을 선택하면 
데이터 세트에서 훈련된 모델이 이전에 못한 영역으로 얼마나 잘 일반화되는지 평가할 수 있다.
데이터 세트에 대한 여러 최첨단 2D 및 3D 물체 감지 및 추적 방법의 벤치마크 결과를 제시한다.

3. Waymo Open Dataset
3.1. Sensor Specifications
데이터 수집은 5개의 LiDAR 센서와 5개의 고해상도 핀홀 카메라를 사용하여 수행되었다.
LiDAR 데이터의 범위를 제한하고 각 레이저 펄스의 처음 두 반환에 대한 데이터를 제공한다.
전면(F), 전면-좌측(FL), 전면-우측(FR), 좌측(SL), 우측(SR) 카메라
이미지 크기는 원본 센서 데이터를 자르고 다운샘플링한 결과를 반영
카메라 수평 시야(HFOV)는 카메라 센서 프레임의 x-y 평면에서 x축의 각도 범위로 제공

3.2 Coordinate System
이 섹션에서는 데이터셋에 사용된 좌표계에 대해 설명한다.
모든 좌표계는 오른손 법칙을 따르며, 데이터셋에는 런 세그먼트 내의 두 프레임 간에 데이터를 변환하는데 필요한 모든 정보가 포함되어 있다.
글로벌 프레임은 차량이 움직이기 전에 설정된다.
이것은 East-North-Up 좌표계이다.
차량 프레임은 차량과 함께 움직인다.
x축은 앞으로 양수, y축은 왼쪽으로 양수, z축은 위쪽으로 양수이다.
LiDAR 구면 좌표계는 LiDAR센서 프레임의 데카르트 좌표계를 기반으로 한다.

3.3. Ground Truth Labels
당사는 LiDAR 센서 판독값과 카메라 이미지 모두에 대해 고품질 실측 자료 주석을 제공한다.
모든 라벨에 대해 길이, 너비, 높이를 각각 x축, y축 및 z축의 크기로 정의한다.
각 객체를 고유한 추적 ID를 가진 7-DOF 3D 수직 경계 상자로 레이블로 지정했으며, 여기서 cx, cy, cz는 중심 좌표, I,w,h는 길이, 너비, 높이
a는 경계 상자의 방향 각도를 나타낸다.
우리는 3D 상자와 그들의 amodal 2D 프로젝션을 보완하는 꼭 맞는 4-DOF 이미지 축 정렬 2D 바운딩 박스로 각 오브젝트에 주석을 달았다.
kitti와 유사하게 난이도 등급에 대해 두 가지 수준을 사용하며, 여기서 LEVEL2에 대한 메트릭은 누적되므로 LEVEL1을 포함한다.

3D LiDAR Detection
모든 LiDAR가 포함된 단일 프레임의 센서 데이터에 대해 모델을 훈련했다.
차량과 보행자의 경우 복셀 크기를 0.33m로, 그리드 범위를 X축, Y축을 따라 [-85m, 85m], Z축을 따라 [-3m, 3m]로 설정한다.
이를 통해 512x512 픽셀의 BEV psuedo image를 얻을 수 있다.











