Abstract
네트워크는 동일한 토폴로지로 일련의 변환을 집계하는 빌딩 블록을 반복하여 구성된다. 
(토폴로지 : 신경망의 레이어들(층)과 그 레이어들 간의 연결 방식을 의미)

이 디자인에서 중요한 요소로 "카디널리티"라는 새로운 차원을 제시한다. 
카디널리티는 변환 집합의 크기를 의미하며, 기존의 네트워크 차원인 깊이(Depth)와 너비(Width)에 추가되는 중요한 요소이다.
카디널리티를 증가시키는 것은 네트워크의 복잡성을 유지하면서도 분류 정확도를 향상시키는 데 도움이 된다. 연구 결과에 따르면, 카디널리티를 늘리는 것이 깊이(더 깊은 네트워크를 만드는 것)나 너비(더 넓은 네트워크를 만드는 것)를 늘리는 것보다 효율적이다.

ImageNet-1K 데이터 세트에서 복잡성을 유지하는 제한된 조건에서도 카디널리티를 늘리면 분류 정확도가 향상될 수 있음을 경험적으로 보여준다.
카디널리티를 늘리는 것이 용량을 늘릴 때 더 깊거나 넓게 가는 것보다 더 효과적이다. 

1. Introduction
대규모 데이터에서 신경망이 학습한 기능은 훈련 중에 최소한의 인간 개입이 필요하며 다양한 인식 작업으로 이전될 수 있다.
그럼에도 불구하고 인간의 노력은 표현 학습을 위한 더 나은 네트워크 아키텍쳐를 설계하는 것으로 전환되었다.
아키텍쳐를 설계하는 것은 하이퍼 파라미터의 수가 증가함에 따라 점점 더 어려워지고 있으며, 특히 계층이 많은 경우 더욱 그렇다.
VGG-nets는 매우 깊은 네트워크를 구성하는 간단하면서도 효과적인 전략을 보여준다. 
: stacking building blocks of the same shape
ResNet(Residual Network)는 동일한 토폴로지를 가진 모듈들을 반복적으로 쌓아올리는 전략을 사용한다. 
이 전략은 하이퍼파라미터 선택의 자유도를 줄여주고, 네트워크의 깊이(Depth)를 중요한 차원으로 노출시킨다.
이 규칙의 단순성이 하이퍼파라미터를 특정 데이터 세트에 과도하게 적용하는 위험을 줄일 수 있다고 주장한다.
ResNet과 VGG 네트워크의 견고함은 다양한 시각적 인식 작업뿐만 아니라, 음성 인식 및 언어 처리와 같은 비시각적 작업에서도 입증되었다.

VGG-nets와 달리 inception 모델은 신중하게 설계된 토폴로지가 낮은 이론적 복잡성을 뛰어난 정확도를 달성할 수 있음을 보여주었다. 
inception 모델의 중요한 공통 속성은 분할-변환-병합 전략이다. 
iinception 모듈에서 입력은 몇 가지 저차원 임베딩으로 분활되고, 특수 필터 세트에 의해 변환되고, 연결로 병합된다.
Split-Transform-Merge 전략
Inception 모듈에서는 입력을 여러 저차원 임베딩으로 나눈 후, 각각을 특화된 필터(예: 1×1, 3×3, 5×5 필터)로 변환하고, 마지막에 이들을 병합한다.
Inception 모듈은 큰 단일 필터를 사용하는 것보다 효율성이 높으면서도 유사한 성능을 낼 수 있도록 설계되었다.
좋은 정확도에도 불구하고, 설계 과정에서 복잡한 요소들이 많아 새로운 데이터셋이나 작업에 적응하기가 어렵다.

이 논문에서는 VGG/ResNets의 반복 레이어 전략을 채택하는 동시에 분할-변환-병합 전략을 쉽고 확장 가능한 방식으로 활용하는 간단한 아키텍쳐를 제시한다.
네트워크의 모듈은 각각 저차원 임베딩에서 일련의 변환을 수행하며, 출력은 합계에 의해 집계된다. 이 모델에 두 가지 다른 동등한 형태가 있다.

우리의 모듈은 모든 경로가 동일한 토폴로지를 공유하므로 경로 수가 조사할 요소로 쉽게 격리할 수 있다는 점에서 기존의 모든 인셉션 모듈과 다르다.
보다 간결하게 재구성하자면, 우리의 모듈은 group convolution에 의해 재구성 될 수 있지만, 이는 엔지니어링 절충안으로 개발되었다.
(Grouped convolutions은 입력 채널을 여러 그룹으로 나누어 각 그룹에 대해 독립적으로 컨볼루션을 수행한 후, 결과를 합치는 방식. 이를 통해 계산 복잡도를 줄이면서도 성능을 유지할 수 있다.)
(FLOP: 모델이 수행하는 총 연산의 수)
용량을 늘려 정확도를 높이는 것은 상대적으로 쉽지만 복잡성을 유지 또는 줄이면서 정확도를 높이는 방법은 드물다.
카디널리티가 너비와 깊이의 차원 외에도 매우 중요한 구체적이고 측정 가능한 차원임을 나타낸다. 
실험에 따르면, 깊이(depth)나 너비(width)를 증가시키는 것보다 카디널리티를 증가시키는 것이 더 효과적인 성능 향상을 가져온다는 것이다.
특히, 깊이와 너비를 증가시키는 것이 성능 향상에 점차적으로 효과가 줄어드는 상황에서는, 카디널리티를 늘리는 것이 더 좋은 결과를 낼 수 있다.

2. Related Work
Multi-branch convolutional network
inception model은 각 브랜치가 커스터마이징되는 성공적인 다중 브랜치 아키텍쳐이다. 

Grouped convolutions
컨볼루션 신경망에서 입력 채널을 여러 그룹으로 나눠서 각각의 그룹에 대해 독립적인 컨볼루션을 수행하는 방법이다.
예를 들어, 입력이 64개의 채널로 구성되어 있다면, 이를 2개의 그룹으로 나눠 각 그룹에 대해 컨볼루션을 수행할 수 있다. 이는 전체 입력을 대상으로 하나의 큰 컨볼루션을 수행하는 것보다 계산량이 줄어든다.
그러나 최근까지는 이 방법이 정확도를 향상시키기 위해 적극적으로 사용된 예는 많지 않다.
Channel-wise Convolutions는 Grouped Convolutions의 한 특수한 경우로, 그룹의 수가 입력 채널의 수와 같은 경우를 말한다. 이때 각 채널은 독립적으로 컨볼루션을 수행하게 된다.
이러한 방식은 Separable Convolutions라는 개념의 일부로 사용되며, 이는 계산 효율성을 높이기 위해 채택된 방법이다.

Compressing convolutional networks
Decomposition(분해)방법은 컨볼루션 신경망의 공간적(spatial) 또는 채널적(channel) 차원을 분해함으로써 신경망의 중복성을 줄이고, 계산 효율성을 높이는 기술이다.
이를 통해 모델의 복잡성을 줄이고, 더 작은 모델 크기로 가속화할 수 있다.
Root-Patterned Network : Ioannou 등이 제안한 "루트-패턴 네트워크"는 계산량을 줄이기 위해 "루트" 구조를 기반으로 여러 분기를 만들어 grouped convolutions(그룹화된 컨볼루션)를 적용하는 방식이다. 

Ensembling
독립적으로 훈련된 네트워크 세트의 평균을 구하는 것은 정확도를 향상시키는 효과적인 솔루션이다.
Viet은 단일 ResNet을 얕은 네트워크의 앙상블로 해석한다. 
하지만 이 논문의 방법을 앙상블로 보는 것은 부정확하다고 주장한다. 그 이유는 집계될 구성원들이 독집적으로 훈련되는 것이 아니라 
공동으로 훈련되기 때문이다. 

3. Method

