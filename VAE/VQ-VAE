## Neural Discrete Representation Learning

## Abstract
이산 표현을 학습하는 간단하지만 강력한 생성 모델을 제안
VQ-VAE는 두가지 주요 면에서 VAE와 다르다.

(autoregressive decoder : 데이터를 순차적으로 생성하는 강력한 디코더이다. 
예를 들어, 문장이나 이미지를 생성할 때 이전에 생성된 요소를 바탕으로 다음 요소를 예측하고 생성한다.)
autoregressive decoder는 매우 강력하기 때문에, 인코더가 생성한 잠재 변수를 무시하고도 충분히 좋은 데이터를 생성할 수 있다. 
이 경우 인코더가 학습 과정에서 잠재 변수를 제대로 활용하지 않게 되어, 잠재 변수가 의미를 상실하는 '사후 붕괴' 현상이 발생한다.

1. Introduction
원시 데이터(예: 이미지, 텍스트, 음성 등)의 중요한 특징들을 추출하여 이를 간단한 형태로 변환한다. 
어려운 작업을 잘 수행하려면, 데이터로부터 학습된 표현이 매우 중요하지만 
비지도 학습 방식으로 훈련된 표현은 여전히 머신러닝 분야에서 주류 접근 방식이 되지 못하고 있다.

최대 우도(Maximum Likelihood): 최대 우도는 모델이 주어진 데이터를 가장 잘 설명할 수 있도록 모델의 파라미터를 조정하는 방법
예를 들어, 이미지 생성 모델이 특정 이미지를 생성할 확률이 최대가 되도록 학습시키는 것이다.
쉽게 말해, 모델이 학습 데이터를 관찰할 가능성(likelihood)을 최대화하는 방향으로 학습한다.

재구성 오류(Reconstruction Error): 재구성 오류는 입력 데이터를 모델에 넣어서 다시 원래 데이터로 복원했을 때의 차이를 측정하는 것
예를 들어, 오토인코더(autoencoder) 모델은 이미지를 압축한 후 다시 복원한다. 원래 이미지와 복원된 이미지 사이의 차이가 재구성 오류이다.
재구성 오류를 최소화하는 것은 모델이 원래 데이터의 중요한 특징을 잘 유지하도록 하는 목표이다.

 이 방법들이 얼마나 유용한지는 그 모델이 어떤 응용 프로그램에 사용되는지에 따라 달라질 수 있다.
예를 들어, 이미지 생성 작업에서는 최대 우도가 중요할 수 있다. 왜냐하면, 모델이 생성한 이미지가 실제 데이터와 최대한 유사해야 하기 때문이다.
반면에, 데이터 압축이나 노이즈 제거와 같은 응용에서는 재구성 오류를 최소화하는 것이 더 중요할 수 있다. 원래 데이터를 얼마나 정확하게 복원할 수 있는지가 관건이기 때문이다.

-> 목표는 잠재 공간에서 데이터의 중요한 기능을 보존하면서 최대한의 가능성을 위해 최적화하는 모델을 달성하는 것

PixelCNN 같은 모델은 잠재 변수를 사용하지 않고도 훌륭한 성능을 낼 수 있는 생성 모델이다. 
하지만 다양한 영여겡서 불연속적이고 유용한 잠재 변수를 학습해야 한다. 즉, 이산 표현에 집중해야 한다.
이산 표현은 복잡한 추론, 계획 및 예측 학습에 자연스럽게 적합하다. 

VQ-VAE는 잠재 공간을 효과적으로 활용할 수 있기 때문에 국소적인 노이즈와 인지할 수 없는 세부 사항에 집중하는 것과는
대조적으로 데이터 공간의 여러 차원에 걸쳐 있는 중요한 특징을 성공적으로 모델링할 수 있다. 

2. Related Work
discrete latent variables를 사용하여 variational autoencoders를 학습하는 방법을 제시한다.
discrete variables를 딥러닝에 적용하는 것은 어려운 것으로 입증되어있다. - 기본 양식이 본질적으로 불연속적인 경우에도

(변분 오토인코더(VAE)에서의 목표 함수는 주로 ELBO를 최적화하는 것이다. 
single-sample objective : VAE에서 학습할 때, latent variable을 한 번 샘플링하여 이 샘플로부터 목표 함수를 계산하는 방법
이 방법은 계산이 단순하지만, 샘플링된 z에 따라 결과가 크게 달라질 수 있다. 즉, 그라디언트의 변동성(분산)이 커져 학습이 불안정해질 수 있다.
multi-sample objective : 잠재 변수 z를 여러 번 샘플링하고, 각 샘플에 대해 목표 함수를 계산한 후, 이를 평균내어 최종 목표 값을 계산한다.)

이산적 잠재 변수와 학습의 어려움
잠재 변수가 이산적(discrete)일 경우, 그라디언트 기반의 최적화가 어려워진다. 이산 변수는 갑작스러운 변화를 가지며, 미분 가능하지 않기 때문에 그라디언트를 계산하기 어렵다.
Continuous Reparameterization : 재매개변수화는 모델이 이산적인 문제를 연속적인 문제로 바꿔서 처리하는 기법이다.
Concrete, Gumbel-softmax 분포
검벨-소프트맥스(Gumbel-softmax)는 검벨(Gumbel) 분포에서 샘플링한 값들에 소프트맥스(softmax) 함수를 적용하여 얻는 분포이다.
이를 통해 이산적 선택 문제를 연속적 값으로 근사할 수 있다.
콘크리트 분포는 이와 유사하게 이산적 변수를 연속적으로 표현하여 학습 가능한 형태로 만든다.

온도 상수는 확률 분포의 "부드러움"을 조절한다. 온도가 높을수록 분포는 더 부드러워지고, 샘플들이 더 균등하게 분포된다. 반면, 온도가 낮아질수록 분포는 날카로워지고, 특정 값에 더 집중된다.
학습 과정에서 온도 상수를 점차 낮추는 과정을 '어닐링(annealing)'이라고 한다. 

Gaussian reparameterization trick은 연속적인 잠재 변수에 대해 그라디언트를 효율적으로 계산할 수 있게 한다. 이를 통해 모델의 학습 과정에서 발생하는 그라디언트의 분산(gradient variance)이 매우 작아져, 더 안정적인 학습이 가능하다.

Scalar Quantization는 데이터를 압축하는 기본적인 방법 중 하나이다. 양자화는 데이터를 일정한 범위 내의 값들로 근사하는 과정으로, 이 과정을 통해 데이터의 크기를 줄이거나 압축할 수 있다.
스칼라 양자화는 데이터를 특정한 범위나 단계(step)로 나누고, 각 데이터를 해당 범위의 대표 값으로 변환한다.

Arithmetic Encoding은 데이터를 매우 효율적으로 압축할 수 있는 엔트로피 부호화(entropy coding) 기법 중 하나이다.
특히 데이터의 발생 확률에 따라 데이터를 가변 길이의 비트열로 인코딩하는 방법이다. 발생 확률이 높은 데이터는 짧은 비트열로, 발생 확률이 낮은 데이터는 긴 비트열로 표현되어 전체적으로 압축이 잘 된다.

Vector Quantization 과정 
코드북(Codebook) : 벡터 양자화의 핵심은 코드북이라는 데이터베이스를 만드는 것이다. 코드북은 미리 정의된 벡터들의 집합으로, 원본 데이터 벡터들을 가장 잘 대표할 수 있는 벡터들의 모음이다.
양자화(Quantization): 벡터 양자화에서는 원본 벡터를 코드북에 있는 가장 가까운 벡터로 대체한다. 이를 통해 원래의 벡터 공간을 코드북에 있는 벡터들로 분할하여, 각각의 입력 벡터를 가장 가까운 코드북 벡터로 매핑다.
압축(Compression): 벡터 양자화 과정을 통해 입력 데이터는 코드북의 인덱스로 대체되므로, 데이터가 압축된다. 즉, 원래 데이터 대신 코드북에서 선택된 벡터의 인덱스만 저장한다.

hard clustering :  각 데이터 포인트가 하나의 클러스터에만 속할 수 있다. 즉, 데이터 포인트가 특정 클러스터에 할당되면, 다른 클러스터에는 속할 수 없다.
어닐링(annealing) 과정은 벡터 양자화에서 사용된 연속적 이완(continuous relaxation)을 점차 하드 클러스터링(hard clustering)으로 변환하는 과정을 의미한다.

오토인코더와 벡터 양자화를 결합하여 신경망을 훈련하는 방법
1. 오토인코더 학습
2. 벡터 양자화(Vector Quantization) 적용 - 인코더의 출력을 사전에 정의된 코드북의 벡터들로 양자화하는 과정
3. Soft-to-Hard Relaxation - 네트워크 전체를 작은 학습률로 다시 훈련시켜, 소프트에서 하드로 전환하는 과정을 안정화

실험에서의 문제:
실험 과정에서 저자들이 발견한 문제는 디코더가 연속적인 이완(continuous relaxation)을 역전시킬 수 있다는 것이었다.
(즉, 양자화가 제대로 작동하지 않았고, 디코더가 연속적인 값으로 돌아가 버렸기 때문에 실제로 원하는 대로 이산적 데이터를 만들지 못했다는 의미이다.)

3. VQ-VAE
